---
title: "[论文解读]learning to navigate the web"
author: lornd
mathjax: true
date: 2023-08-11 09:50:32
url: qweb
tags:
---

论文地址：[learning to navigate the web](https://arxiv.org/abs/1812.09195) 。

## 摘要

在有着巨大的状态空间和动作空间，以及稀疏奖励的环境中进行学习，会阻碍强化学习智能体通过试错的学习过程。例如，在网页上遵循自然语言命令执行操作（如预定机票），会创造一个输入词汇量和单个页面上可操作元素数量非常庞大的强化学习环境。即使最近的研究通过人类演示引导探索，在相对简单的环境中提升了成功率，在那些可能有成千上万条可能的指令的环境中，这些方法还是会失败。**我们尝试从一个不同的角度解决前面提到的问题，并提出了一个可以生成无限用于智能体学习的经验的强化学习引导方法。**我们不让智能体从含有大量词汇的复杂指令中进行学习，而是将这些指令分解成多个子指令，并且规划了一个“课程表”，使得智能体在一个递增的相对容易的子指令集中进行学习。除此之外，当专家演示不可用时，我们提出了一个理想的元学习框架以生成新的指令跟随任务，并且更有效地训练智能体。我们使用一种新的 QWeb 神经网络架构在这些较小的合成指令上近似 Q 值函数，并基于此训练 DQN 深度强化学习智能体。我们在 WoB 环境上测试我们智能体泛化到新的指令的能力。并且在超过 100 个元素，支持超过 1400 万种可能的命令的表单上进行测试。QWeb 智能体在一些困难的环境中，不使用任何人类演示，但是表现超过了 baseline，达到了 $100\%$ 的准确率。


## 研究背景

在一个经典的 web 环境中，一个智能体需要仔细浏览大量的网络元素，以遵循从大量词汇中构建的高度变化的指令。例如对于指令“Book a flight from WTK to LON on 21-Oct-2016”，智能体可以以任何顺序填写前三个字段；选择的选项很多，但是在所有可能的机场/日期组合中，只有一个是正确的；只有在填写完所有的三个字段之后，才能提交表单；在这之后，网络环境/网页会发生变化，才可以选择航班。因此这个任务是十分困难的。在这样的任务中，通过试错去达到最终的目标是繁琐的，并且稀疏奖励会导致大多数 episode 根本不会产生任何信号。并且当智能体从大量指令中学习时，由于不一定能访问到每个选项，这个问题会更加严重。

对于这个问题，一个常见的解决方法是：通过从人类演示中学习，以及使用预先训练好的词嵌入来引导探索走向更有价值的状态。但是前人的工作对于每个环境使用了单独的演示，随着环境的复杂性增加，这些方法无法产生任何成功的 episode 。并且，在有着巨大动作和状态空间的环境中，收集人类演示这一行为无法扩展，因为对于每个环境，需要大量的人类演示用于训练。

## 文章贡献

本文提出了两种网页导航中大型状态和动作空间，且奖励稀疏的强化学习方法。当有专家演示或者一种指令遵循策略（ORACLE）时，我们提出了 curriculum-DQN，这是一个课程学习方法，引导探索从一些更简单的命令遵循任务开始，在大量的训练中逐渐增加难度。curriculum-DQN 将一个命令分解成多个子命令，并且将这些子指令集中的更简单的任务分配给导航智能体去做。一个专家指令遵顼策略（ORACLE）能够让智能体更加靠近其目标。

当专家演示和 ORACLE 策略不可用的时候，我们提出了一个新的**元学习框架**，利用无指令的任意网络导航策略，训练一个专家指令遵循演示的生成式模型。这其中的关键点在于：我们可以将一个任意的导航策略（如随机策略）视为某些隐藏指令的专家指令遵循策略。如果我们能够复原这些隐藏指令，那么我们就可以自动生成新的专家演示，并且用他们来提升导航智能体的训练。直观上来说，从策略中生成指令比遵循指令更加容易，因为导航智能体不需要和动态网页交互并且采取复杂的行动。因此，我们开发了一个**指导智能体** meta-trainer ，通过生成新的专家演示来训练导航智能体。

除了两种训练方法，本文还提出了两个新的神经网络结构 QWeb 和 INET 用于嵌入网页导航中的 Q 值函数,融合了自注意力，LSTM 和浅编码。QWeb 是用于指令遵循策略的 Q 值函数，可以和 curriculum-DQN 或指导智能体。而 INET 是指导智能体的 Q 值函数。我们在 Miniwob 和 Miniwob++ 数据集上测试了我们方法的性能，发现这两种方法都改进了一个强基线，并且超过了先前最先进的技术。

同时，本文所提出的方法，基于有注意力机制的 DQN 的自动化课程生成可能会被大型任务计划社区内的工作者所感兴趣。因为它们可以帮助解决大型离散状态空间和动作空间马尔可夫决策过程下的目标导向任务。

## 问题描述

给定命令 $I=[F=(K, V)]$ ，其中 $I$ 是字段 $F$ 的列表，每个字段表示为一个键值对 $(K, V)$（如 `{from: ”San Francisco”, to: ”LA”, date: ”12/04/2018”}`）。在每个时间步，**环境的状态** $s_t$ 包括指令 $I$ ，和网页的 DOM 树表示 $D_t$ 。每个 DOM 元素被表示为命名属性（如 tag, value, name, text, id, class 等）的列表。**环境的奖励**是通过比较一个 episode 的最终状态 $D_N$ 和最终目标状态 $G(I)$ 计算得到的。**动作空间**被限制为 `Click(e)` 和 `Type(e, y)` ，其中 $e$ 是 DOM 树上的一个叶子 DOM 元素，$y$ 是指令里面一个字段的值。这两个复合操作主要通过 DOM 元素 $(e)$ 来识别，如文本框是通过序列进行键入，而日期选择器是通过点击进行选择。这样的性质启发我们将复合动作表示为原子动作的层次结构，如图 2 所示：

{% gallery %}
![原子操作层次图](/images/learning-to-navigate-web/fig2.png)
{% endgallery %}

在这个框架下，考虑节点之间的依赖关系，分别对图中的节点进行建模，再组合起来定义组合 Q 值函数：

$$Q(s, a) = Q(s, a_D) + Q(s, a_c | a_D) + Q(s, a_T|a_D,[a_c=="{\rm type}"])$$

其中 $a=(a_D,a_C,a_T)$ 是组合动作，$a_D$ 表示选择一个 DOM 元素，$a_C|a_D$ 表示在选定的 DOM 元素上的点击或输入操作，$a_T|a_D,[a_c=="{\rm type}"]$ 表示在选定的 DOM 元素上输入命令中的一个序列。在执行策略时（在探索的过程中或者测试的过程中），智能体首先选择具有最高的 $Q(s, a_D)$ 的 DOM 元素；然后基于 $Q(s, a_C|a_D)$ 在选择的 DOM 元素上选择输入或点击操作；对于输入操作，智能体利用 $Q(s, a_T|a_D,[a_c=="{\rm type}"])$ 从命令中选择一个值。

## 解决方法

### 用于网页导航的引导型 Q 学习

在本节中，首先介绍我们的深度 Q 网络，即 QWeb ，用于生成每个给定的状态 $(s_t = (I, D_t))$ 和每个原子操作 $a_D,a_C,a_T$ 的 Q 值。接下来将解释我们如和使用 DOM 和命令的浅编码层扩展这个网络以解决学习一个巨大的输入词典的问题。最后，我们深入研究了我们的奖励增强和课程学习方法以解决前面提到的问题。

#### 用于网页导航的深度 Q 网络（QWeb）

{% gallery %}
![网络结构图](/images/learning-to-navigate-web/fig3.png)
{% endgallery %}

如图 3a 所示，QWeb 由三个不同的层组成，这三个不同的层以层次结构的形式连接，每个层对给定状态的不同部分进行编码操作：

- 编码用户命令。
- 编码 DOM 元素属性和用户命令中重合的文字部分。
- 编码 DOM 树。

给定一个命令 $I=[F=(K,V)]$ ，QWeb 首先通过学习每个 $K$ 和 $V$ 的嵌入，将每个字段 $F$ 编码成一个固定长度的向量。DOM 元素属性和用户命令中重合的文字序列也被编码成一个单独的向量，以在相似上下文的情景下描述每个元素的情况。最后 DOM 树的编码是通过线性化树形结构之后，在 DOM 元素序列上应用一个双向 LSTM 网络（biLSTM）实现。LSTM 网络的输出和命令字段的编码会被用来生成每个原子动作的 Q 值。

**用户命令编码** 我们将一个命令表示为一个向量的列表，其中每个向量对应一个不同的命令字段。一个字段的编码过程为：首先编码对应的键和值，然后将组合起来的编码通过一个有着 ReLU 激活函数的全连接层（FC）进行转换，得到最终的编码。假设 $E^f_K(i, j), E^f_V(i, j)$ 分别表示第 $i$ 个字段的键和值的第 $j$ 个单词的嵌入表示。那么键和值的嵌入表示就是其单词嵌入的平均值，如 $E^f_K(i) = \dfrac{1}{|F(i)|}\sum_j E^f_K(i, j)$ 是一个键的嵌入表示。一个字段的嵌入通过公式 $E^f(i) = FC([E_K(i),E_V(i)])$ 计算，其中 $[,]$ 表示向量的拼接。

**DOM-指令交叉编码** 对于命令中的每个字段和一个 DOM 元素的每个属性，生成一个重复单词的序列。通过并行编码这些序列，我们能够生成命令导向的 DOM 元素编码。我们对每个序列和每个属性的词嵌入取平均，以计算在每个命令字段的条件下 DOM 元素的嵌入。我们使用自注意力机制计算指令字段上的概率分布，并将这个命令导向的嵌入降维成一个单独的 DOM 元素编码。假设 $E(f,D_t(i))$ 表示在字段 $f$ 的条件下 DOM 元素的嵌入表示，其中 $D_t(i)$ 是第 $i$ 个 DOM 元素。$D_t(i)$ 的条件嵌入是所有这些嵌入的加权平均，即 $E_C = \sum_fp_f\times E(f, D_t(i))$ ，其中自注意力概率的计算方式为：$p_f = softmax_i(u \times E^f)$ ，其中 $u$ 是可训练参数。

**DOM 树编码** 首先将属性中的每个词嵌入取平均得到属性的嵌入表示。每个 DOM 元素的编码就是其属性嵌入的平均值。给定条件 DOM 元素编码，我们将它们和 DOM 元素嵌入连接起来以为每个 DOM 元素生成一个单独的向量。接下来在 DOM 元素嵌入列表之上应用一个双向 LSTM 网络（biLSTM）以对 DOM 树进行编码。biLSTM 的每一个输出向量之后会通过一个带有 tanh 激活函数的全连接层进行转换，以生成最终的 DOM 元素表示。

**生成 Q 值** 给定指令中每个字段的编码和 DOM 树上每个 DOM 元素的编码，我们计算每个字段和每个 DOM 元素之间的两两相似性，得到一个上下文矩阵 $M$ 。$M$ 的行和列分别表示当前状态下每个字段和每个 DOM 元素的后验值。通过应用一个全连接层，并且对 $M$ 的行求和，可以为每一个 DOM 元素生成 Q 值 $Q(s_t,a_t^D)$ 。我们使用 $M$ 的行作为将指令的字段输入一个 DOM 元素的 $Q$ 值，即 $Q(s_t,a_t^T) = M$ 。最后，点击或输入动作的 Q 值是通过将 $M$ 的行应用另一个全连接层转换为 $2$ 维向量计算得到，即 $Q(s_t,a_t^C)$ 。组合动作 $a_t$ 的最终 Q 值就是这些 Q 值之和：$Q(s_t,a_t) = Q(s_t,a_t^D) + Q(s_t,a_t^T) + Q(s_t,a_t^C)$ 。

**结合浅编码** 在奖励稀疏且输入词汇量庞大的情况下，仅仅使用词嵌入很难学习到良好的语义相似性。我们使用命令和 DOM 树的浅编码来增强我们的深度 Q 网络，以缓解这个问题。首先通过计算每个命令字段和每个 DOM 元素属性之间的基于词的相似性（如 Jaccard 相似性，子集二进制指示器或者超集二进制指示器）生成联合浅编码矩阵。我们还将每个 DOM 元素兄弟节点的浅编码附加在一起，以明确地结合 DOM 层次结构。我们分别对浅编码矩阵的列和行求和，以生成 DOM 元素和命令字段的浅层输入向量。这些向量通过一个具有 tanh 激活函数的全连接层进行变换，并且通过一个可训练的参数进行缩放，以生成 DOM 元素和命令字段的浅层 Q 值。通过在深度 Q 值和浅层 Q 值之间应用门控机制，可以按照如下方式计算最终的 Q 值：

$$
\hat Q(s, a) = Q_{deep}(s,a_t^D)(1 - \sigma(u)) + Q_{shallow}(s, a_t^D)(\sigma(u)) \\
\hat Q(s, a) = Q_{deep}(s,a_t^T)(1 - \sigma(v)) + Q_{shallow}(s, a_t^T)(\sigma(v))
$$

其中 $u$ 和 $v$ 是可训练参数。

#### 奖励增强

我们使用基于潜力的奖励来增强环境中的奖励函数。因为环境奖励是通过评估最终状态是否完全等价于目标状态来计算的，我们定义潜力函数 $Potential(s, g)$ 计算给定状态 $s$ 和目标状态 $g$ 之间匹配的 DOM 元素的数量，并通过目标状态的 DOM 元素的数量进行归一化。基于潜力的奖励就是根据下一个状态和当前状态的潜力差异进行计算的：

$$R_{potential} = \gamma(Potential(s_{t+1}, g) - Potential(s_t, g))$$

#### 课程学习

我们通过将一个命令分解成多个子命令，并且为智能体分配一些更简单的任务，即仅解决子命令的一个子集，来进行课程学习。我们使用两种不同的课程学习策略来训练 QWeb ：热身策略和子目标模拟策略。

{% gallery %}
![算法 1](/images/learning-to-navigate-web/alg1.png)
{% endgallery %}

**热身策略** 如算法 1 中所示：为了在一个 episode 中进行热身，我们将智能体放置于距离目标状态更近的状态中，这样智能体就只需要学习遵循一小部分子命令就能够成功完成这个 episode 。我们以一个概率 $p$ 独立地访问每个 DOM 元素，并通过 ORACLE 策略来对选择的元素进行正确的操作。在热身过程中，QWeb 的环境被初始化为最终状态，并且原始目标保持不变。并且在训练的开始，$p$ 值最开始设置为一个较大的值（如 $0.85$），并且在预定的部署内逐渐减少为 $0.0$ 。超过这个限制之后，环境的初始状态将恢复为完整指令的普通 DOM 树原始状态。

## 实验验证
