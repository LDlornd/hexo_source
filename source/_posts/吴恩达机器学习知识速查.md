---
title: 吴恩达机器学习知识速查
author: lornd
mathjax: true
date: 2023-02-19 15:52:09
url: MLofAndrew
tags:
pin: true
---

## 第一章 绪论

### 机器学习

Arthur Samuel 的定义：在不被明确定义的情况下，给予计算机学习的能力的研究领域。

Tom Mitchell 的定义：计算机从经验 $E$ 中学习，解决某一任务 $T$ ，进行性能度量 $P$ 。通过 $P$ 评测其在任务 $T$ 上的表现，这个表现会因为 $E$ 而提高。

常见的机器学习算法：监督学习、无监督学习。

### 监督学习

监督学习给予机器学习算法一个包含“正确答案”的数据集，即训练集中含有标签（label）。

监督学习可以被分为两类问题：

- **回归问题**：预测连续值，如预测房价。
- **分类问题**：预测离散值，如预测肿瘤是否良性。

### 无监督学习

无监督学习仅仅给予机器学习算法一个数据集，不包含标签，要求算法从数据中发掘数据的结构。这种问题被称作为聚类问题。

无监督学习的常见应用：

- **新闻聚类**：将讨论同一件事新闻放到一起。
- **基因工程**：判断某一基因在不同的人身上表达的程度。
- **计算集群组织**：通过聚类算法判断哪些机器趋向于协同工作，将这些机器放在一起有助于节省资源。
- **社交网络分析**：识别同一个圈子中的朋友，并且判断哪些人互相认识。
- **市场细分**：将公司客户划分到不同的细分市场，以进行更加精准的推荐。
- **天文分析**：帮助探索星系的形成。
- **音频分离**：鸡尾酒聚会问题。

## 第二章 单元线性回归

### 模型表示

在训练集中，定义一些符号：

- $m$：训练集的大小。
- $x$：输入变量 / 特征。
- $y$：输出变量 / 标签。

基于上述符号，可以使用 $(x, y)$ 来表示一个训练样本。具体地，使用上标 $(x^{(i)}, y^{(i)})$ 来表示第 $i$ 个训练样本。

学习算法的目的是根据给定的数据集，找到一个假设函数 $h$ ，其输入为 $x$ ，输出为 $y$ 。在单元线性回归中，假设 $y$ 与 $x$ 呈线性关系，即 $h_{\theta}(x) = \theta_0 + \theta_1 x$ ，其中 $h_{\theta}(x)$ 表示 $h$ 是关于 $x$ 的函数，可训练参数为 $\theta$ ，有的时候，$h_{\theta}(x)$ 也会被简写成 $h(x)$ 。

### 损失函数

给定训练集，要求找到一个合理的 $h$ ，因此如何确定 $\theta$ 的值就成为了一个重要的问题。一个直观的想法是：选择对于每个训练样本 $(x, y)$ 都能让 $h(x)$ 与 $y$ 接近的 $\theta$ 。因此通过需要一个函数 $J(\theta)$ 来度量 $h(x)$ 与 $y$ 的接近程度。

$$J(\theta) = \dfrac{1}{2m}\sum\limits_{i=1}^m \left(h_{\theta}(x^{(i)}) - y^{(i)}\right)^2$$

其中平方是为了将负数的距离变成正数，$\dfrac{1}{m}$ 是对所有训练样本取平均，$\dfrac{1}{2}$ 是为了方便求导。这个函数被称作为**损失函数** 。而训练的目标就是最小化损失函数，即：

$$\underset{\theta} { {\rm minimize } }\ J(\theta)$$

### 梯度下降

现在我们拥有一个函数 $J(\theta_0, \theta_1)$ ，我们要求其最小值以及对应的 $\theta$ 值。在没有求最值公式的情况下，一个暴力的方法是：先随机指定 $\theta_0$ 和 $\theta_1$ 的值，接着不断调整它们的值，使得 $J(\theta_0, \theta_1)$ 变小，最后 $J(\theta)$ 会收敛到一个局部最小值。

如何使 $J(\theta)$ 下降的速度最快，那就是往梯度的反方向更新参数了。具体地，重复以下步骤直至收敛：

$$\theta_i := \theta_i - \alpha \dfrac{\partial}{\partial \theta_i}J(\theta), (i = 0, 1, \dots)$$

其中 $\alpha$ 是学习率，用于控制参数更新的幅度和下降的速率。对于梯度下降算法，有两点值得注意：

1. 不同参数需要同步更新，以保证所有参数是根据同一个梯度进行更新。
2. 不同的初始值可能导致最后收敛到不同的局部最优值。

学习率对梯度下降的影响：

- 如果学习率过小，会导致梯度下降收敛得很慢。
- 如果学习率过大，会导致损失函数难以收敛，甚至发散。

这里所说的梯度下降算法也被称作为 Batch 梯度下降，Batch 是指每一步梯度下降都遍历了整个训练集。

## 第三章 线性代数

### 矩阵和向量

矩阵是由数字组成的二维数组，其维度被定义为行数 $\times$ 列数。对于矩阵 $A$ ，用 $A_{i, j}$ 表示矩阵 $A$ 中第 $i$ 行第 $j$ 列的元素。

向量是一个 $n\times 1$ 的矩阵，其维度就是 $n$ 维。对于向量 $y$ ，用 $y_i$ 表示向量 $y$ 中的第 $i$ 个元素。

### 矩阵的加法与数乘

只有两个同型矩阵（维度相同）才能相加。两个 $n\times m$ 的矩阵相加，得到的结果是每个元素对应相加。即若 $C = A + B$ ，那么 $C_{i, j} = A_{i, j} + B_{i, j}$ 。

矩阵的数乘，其结果是矩阵中的每一个元素与标量相乘。即若 $B=\alpha A$ ，那么 $B_{i, j} = \alpha A_{i, j}$ 。

矩阵的加法和数乘均不改变矩阵的形状。

### 矩阵向量乘法

一个矩阵左乘一个向量，向量的维数必须与矩阵的列数相等。即一个 $n\times m$ 的矩阵只能与一个 $m$ 维的向量相乘。结果为一个 $n$ 维向量，其第 $i$ 个元素矩阵的第 $i$ 行与向量逐元素相乘再相加的结果，即如果矩阵 $A$ 与向量 $y$ 相乘得到了向量 $z$ ，那么：

$$z_i = \sum\limits_{j = 1}^{m} A_{i, j}y_j$$

可以将矩阵向量乘法的思想运用到线性函数的计算中，假设 $x_1, x_2, x_3, x_4$ 是四个输入，线性函数是 $h(x) = \theta_0 + \theta_1 x$ ，那么有：

$$
\begin{bmatrix}
    1 & x_1 \\
    1 & x_2 \\
    1 & x_3 \\
    1 & x_4
\end{bmatrix}
\times
\begin{bmatrix}
    \theta_0 \\
    \theta_1 \\
\end{bmatrix}
=
\begin{bmatrix}
    h(x_1) \\
    h(x_2) \\
    h(x_3) \\
    h(x_4)
\end{bmatrix}
$$

通过这样的方式，我们可以将多次线性运算转换为一次矩阵运算。具体可以表示为：

$$\rm Prediction = DataMatrix \times Parameters$$

### 矩阵乘法

两个矩阵相乘，后一个矩阵的行数必须与前一个矩阵的列数相同，得到的矩阵的维度维：前一个矩阵的行数 $\times$ 后一个矩阵的列数。这意味着，一个 $n\times m$ 的矩阵与一个 $m\times o$ 的矩阵相乘，结果为一个 $n\times o$ 的矩阵。

矩阵乘法的计算，可以看成多个矩阵与向量乘法计算的拼接。具体地，将 $m\times o$ 的矩阵的第 $i$ 列提取出来，与 $n\times m$ 的矩阵进行乘法运算，就得到了结果矩阵的第 $i$ 列。将全部 $o$ 列计算完毕之后，拼接起来就得到了最终的计算结果。

同矩阵向量乘法相同。矩阵之间的乘法可以对数据矩阵同时进行多种线性运算，假设有两个输入 $x_1, x_2$ ，两个线性运算 $h_1(x), h_2(x)$，那么有：

$$
\begin{bmatrix}
    1 & x_1 \\
    1 & x_2 \\
\end{bmatrix}
\times
\begin{bmatrix}
    h_1 & h_2
\end{bmatrix}
=
\begin{bmatrix}
    h_1(x_1) & h_2(x_1) \\
    h_1(x_2) & h_2(x_2)\\
\end{bmatrix}
$$

在进行矩阵乘法时，需要注意：**矩阵乘法不满足交换律，但是满足结合律**！

### 矩阵的逆和转置

**单位阵的概念**：$n$ 阶单位阵 $I$ 是一个 $n \times n$ 的，对角线上全为 $1$ ，其余部分为 $0$ 的矩阵，其满足：$A \cdot I = I \cdot A = A$ 。虽然矩阵乘法不满足交换律，但是矩阵和单位阵的乘法是满足交换律的。

**矩阵的逆**：矩阵的逆是一个类似于倒数的概念。只有**方阵**（行数与列数相等的矩阵）才有逆矩阵，矩阵 $A$ 的逆矩阵被记作 $A^{-1}$ 。如果矩阵 $A$ 是方阵且有逆矩阵，那么 $AA^{-1} = A^{-1}A = I$ 。

**矩阵的转置**：将矩阵的行与列交换，成为矩阵的转置。矩阵 $A$ 的转置被记作为 $A^T$。具体地:

$$A^T_{ij} = A_{ji}$$

## 第四章 多元线性回归

### 多特征

假设有 $m$ 个样本，每个样本具有 $n$ 个特征，那么第 $i$ 个样本会被表示为：$A^{(i)}$ ，第 $i$ 个样本的第 $j$ 个特征会被表示为：$A^{(i)}_j$ 。

目标函数被表示为：$h_{\theta}(x) = {\theta}_0 + \sum\limits_{i=1}^n{\theta_i x_i}$ 。

令 $x_0 = 1$ ，那么目标函数变为：$h_{\theta}(x)=\sum\limits_{i=0}^n{\theta_i x_i}$ 。

将参数和特征写成矩阵（列向量）形式，有：$h_{\theta}(x)=\theta^Tx$ 。值得注意的是，此时参数和特征都是 $n+1$ 维的。

### 多元梯度下降法

目标函数：$h_{\theta}(x) = \theta^T x = \sum\limits_{i=0}^n{\theta_i x_i}$ 。

参数：$\theta$（$n+1$ 维向量）

损失函数：$J(\theta) = \dfrac{1}{2m} \sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$ 。

梯度下降更新公式：$\theta_j:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j$ 。

### 特征缩放（归一化）

归一化的目的：让梯度下降得更快。

假设数据都是正数，$\mu$ 为数据的平均值，$s$ 为数据的标准差。

最大值归一化：$x=\dfrac{x}{x_{\max}}$ 。

均值归一化：$x=\dfrac{x - \mu}{x_{\max}}$ 。

均值方差归一化：$x=\dfrac{x - \mu}{s}$ 。

### 学习率

画出 $J(\theta)$ 随训练轮次变化的曲线可以帮助判断梯度下降是否正常运行。如果 $J(\theta)$ 在某一段递增，通常是因为学习率设置得过大而引起，这个时候可以将学习率调小。

学习率对梯度下降的影响：

- 如果学习率过小，会导致梯度下降收敛得很慢。
- 如果学习率过大，会导致损失函数难以收敛，甚至发散。

在实践中，通常需要尝试多种学习率，找到最适合对应任务的学习率。
