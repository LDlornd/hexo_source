---
title: 吴恩达机器学习知识速查
author: lornd
mathjax: true
date: 2023-02-19 15:52:09
url: MLofAndrew
tags:
---

## 第一章

待补充

## 第二章

待补充

## 第三章

待补充

## 第四章 多元线性回归

### 多特征

假设有 $m$ 个样本，每个样本具有 $n$ 个特征，那么第 $i$ 个样本会被表示为：$A^{(i)}$ ，第 $i$ 个样本的第 $j$ 个特征会被表示为：$A^{(i)}_j$ 。

目标函数被表示为：$h_{\theta}(x) = {\theta}_0 + \sum\limits_{i=1}^n{\theta_i x_i}$ 。

令 $x_0 = 1$ ，那么目标函数变为：$h_{\theta}(x)=\sum\limits_{i=0}^n{\theta_i x_i}$ 。

将参数和特征写成矩阵（列向量）形式，有：$h_{\theta}(x)=\theta^Tx$ 。值得注意的是，此时参数和特征都是 $n+1$ 维的。

### 多元梯度下降法

目标函数：$h_{\theta}(x) = \theta^T x = \sum\limits_{i=0}^n{\theta_i x_i}$ 。

参数：$\theta$（$n+1$ 维向量）

损失函数：$J(\theta) = \dfrac{1}{2m} \sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$ 。

梯度下降更新公式：$\theta_j:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j$ 。

### 特征缩放（归一化）

归一化的目的：让梯度下降得更快。

假设数据都是正数，$\mu$ 为数据的平均值，$s$ 为数据的标准差。

最大值归一化：$x=\dfrac{x}{x_{\max}}$ 。

均值归一化：$x=\dfrac{x - \mu}{x_{\max}}$ 。

均值方差归一化：$x=\dfrac{x - \mu}{s}$ 。

### 学习率

画出 $J(\theta)$ 随训练轮次变化的曲线可以帮助判断梯度下降是否正常运行。如果 $J(\theta)$ 在某一段递增，通常是因为学习率设置得过大而引起，这个时候可以将学习率调小。

学习率对梯度下降的影响：

- 如果学习率过小，会导致梯度下降收敛得很慢。
- 如果学习率过大，会导致损失函数难以收敛，甚至发散。

在实践中，通常需要尝试多种学习率，找到最适合对应任务的学习率。
