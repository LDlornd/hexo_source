---
title: 吴恩达机器学习知识速查
author: lornd
mathjax: true
date: 2023-02-19 15:52:09
url: MLofAndrew
tags:
pin: true
---

## 第一章 绪论

### 机器学习

Arthur Samuel 的定义：在不被明确定义的情况下，给予计算机学习的能力的研究领域。

Tom Mitchell 的定义：计算机从经验 $E$ 中学习，解决某一任务 $T$ ，进行性能度量 $P$ 。通过 $P$ 评测其在任务 $T$ 上的表现，这个表现会因为 $E$ 而提高。

常见的机器学习算法：监督学习、无监督学习。

### 监督学习

监督学习给予机器学习算法一个包含“正确答案”的数据集，即训练集中含有标签（label）。

监督学习可以被分为两类问题：

- **回归问题**：预测连续值，如预测房价。
- **分类问题**：预测离散值，如预测肿瘤是否良性。

### 无监督学习

无监督学习仅仅给予机器学习算法一个数据集，不包含标签，要求算法从数据中发掘数据的结构。这种问题被称作为聚类问题。

无监督学习的常见应用：

- **新闻聚类**：将讨论同一件事新闻放到一起。
- **基因工程**：判断某一基因在不同的人身上表达的程度。
- **计算集群组织**：通过聚类算法判断哪些机器趋向于协同工作，将这些机器放在一起有助于节省资源。
- **社交网络分析**：识别同一个圈子中的朋友，并且判断哪些人互相认识。
- **市场细分**：将公司客户划分到不同的细分市场，以进行更加精准的推荐。
- **天文分析**：帮助探索星系的形成。
- **音频分离**：鸡尾酒聚会问题。

## 第二章 单元线性回归

### 模型表示

在训练集中，定义一些符号：

- $m$：训练集的大小。
- $x$：输入变量 / 特征。
- $y$：输出变量 / 标签。

基于上述符号，可以使用 $(x, y)$ 来表示一个训练样本。具体地，使用上标 $(x^{(i)}, y^{(i)})$ 来表示第 $i$ 个训练样本。

学习算法的目的是根据给定的数据集，找到一个假设函数 $h$ ，其输入为 $x$ ，输出为 $y$ 。在单元线性回归中，假设 $y$ 与 $x$ 呈线性关系，即 $h_{\theta}(x) = \theta_0 + \theta_1 x$ ，其中 $h_{\theta}(x)$ 表示 $h$ 是关于 $x$ 的函数，可训练参数为 $\theta$ ，有的时候，$h_{\theta}(x)$ 也会被简写成 $h(x)$ 。

### 损失函数

给定训练集，要求找到一个合理的 $h$ ，因此如何确定 $\theta$ 的值就成为了一个重要的问题。一个直观的想法是：选择对于每个训练样本 $(x, y)$ 都能让 $h(x)$ 与 $y$ 接近的 $\theta$ 。因此通过需要一个函数 $J(\theta)$ 来度量 $h(x)$ 与 $y$ 的接近程度。

$$J(\theta) = \dfrac{1}{2m}\sum\limits_{i=1}^m \left(h_{\theta}(x^{(i)}) - y^{(i)}\right)^2$$

其中平方是为了将负数的距离变成正数，$\dfrac{1}{m}$ 是对所有训练样本取平均，$\dfrac{1}{2}$ 是为了方便求导。这个函数被称作为**损失函数** 。而训练的目标就是最小化损失函数，即：

$$\underset{\theta} { {\rm minimize } }\ J(\theta)$$

### 梯度下降

现在我们拥有一个函数 $J(\theta_0, \theta_1)$ ，我们要求其最小值以及对应的 $\theta$ 值。在没有求最值公式的情况下，一个暴力的方法是：先随机指定 $\theta_0$ 和 $\theta_1$ 的值，接着不断调整它们的值，使得 $J(\theta_0, \theta_1)$ 变小，最后 $J(\theta)$ 会收敛到一个局部最小值。

如何使 $J(\theta)$ 下降的速度最快，那就是往梯度的反方向更新参数了。具体地，重复以下步骤直至收敛：

$$\theta_i := \theta_i - \alpha \dfrac{\partial}{\partial \theta_i}J(\theta), (i = 0, 1, \dots)$$

其中 $\alpha$ 是学习率，用于控制参数更新的幅度和下降的速率。对于梯度下降算法，有两点值得注意：

1. 不同参数需要同步更新，以保证所有参数是根据同一个梯度进行更新。
2. 不同的初始值可能导致最后收敛到不同的局部最优值。

学习率对梯度下降的影响：

- 如果学习率过小，会导致梯度下降收敛得很慢。
- 如果学习率过大，会导致损失函数难以收敛，甚至发散。

这里所说的梯度下降算法也被称作为 Batch 梯度下降，Batch 是指每一步梯度下降都遍历了整个训练集。

## 第三章 线性代数

### 矩阵和向量

矩阵是由数字组成的二维数组，其维度被定义为行数 $\times$ 列数。对于矩阵 $A$ ，用 $A_{i, j}$ 表示矩阵 $A$ 中第 $i$ 行第 $j$ 列的元素。

向量是一个 $n\times 1$ 的矩阵，其维度就是 $n$ 维。对于向量 $y$ ，用 $y_i$ 表示向量 $y$ 中的第 $i$ 个元素。

### 矩阵的加法与数乘

只有两个同型矩阵（维度相同）才能相加。两个 $n\times m$ 的矩阵相加，得到的结果是每个元素对应相加。即若 $C = A + B$ ，那么 $C_{i, j} = A_{i, j} + B_{i, j}$ 。

矩阵的数乘，其结果是矩阵中的每一个元素与标量相乘。即若 $B=\alpha A$ ，那么 $B_{i, j} = \alpha A_{i, j}$ 。

矩阵的加法和数乘均不改变矩阵的形状。

### 矩阵向量乘法

一个矩阵左乘一个向量，向量的维数必须与矩阵的列数相等。即一个 $n\times m$ 的矩阵只能与一个 $m$ 维的向量相乘。结果为一个 $n$ 维向量，其第 $i$ 个元素矩阵的第 $i$ 行与向量逐元素相乘再相加的结果，即如果矩阵 $A$ 与向量 $y$ 相乘得到了向量 $z$ ，那么：

$$z_i = \sum\limits_{j = 1}^{m} A_{i, j}y_j$$

可以将矩阵向量乘法的思想运用到线性函数的计算中，假设 $x_1, x_2, x_3, x_4$ 是四个输入，线性函数是 $h(x) = \theta_0 + \theta_1 x$ ，那么有：

$$
\begin{bmatrix}
    1 & x_1 \\
    1 & x_2 \\
    1 & x_3 \\
    1 & x_4
\end{bmatrix}
\times
\begin{bmatrix}
    \theta_0 \\
    \theta_1 \\
\end{bmatrix}
=
\begin{bmatrix}
    h(x_1) \\
    h(x_2) \\
    h(x_3) \\
    h(x_4)
\end{bmatrix}
$$

通过这样的方式，我们可以将多次线性运算转换为一次矩阵运算。具体可以表示为：

$$\rm Prediction = DataMatrix \times Parameters$$

### 矩阵乘法

两个矩阵相乘，后一个矩阵的行数必须与前一个矩阵的列数相同，得到的矩阵的维度维：前一个矩阵的行数 $\times$ 后一个矩阵的列数。这意味着，一个 $n\times m$ 的矩阵与一个 $m\times o$ 的矩阵相乘，结果为一个 $n\times o$ 的矩阵。

矩阵乘法的计算，可以看成多个矩阵与向量乘法计算的拼接。具体地，将 $m\times o$ 的矩阵的第 $i$ 列提取出来，与 $n\times m$ 的矩阵进行乘法运算，就得到了结果矩阵的第 $i$ 列。将全部 $o$ 列计算完毕之后，拼接起来就得到了最终的计算结果。

同矩阵向量乘法相同。矩阵之间的乘法可以对数据矩阵同时进行多种线性运算，假设有两个输入 $x_1, x_2$ ，两个线性运算 $h_1(x), h_2(x)$，那么有：

$$
\begin{bmatrix}
    1 & x_1 \\
    1 & x_2 \\
\end{bmatrix}
\times
\begin{bmatrix}
    h_1 & h_2
\end{bmatrix}
=
\begin{bmatrix}
    h_1(x_1) & h_2(x_1) \\
    h_1(x_2) & h_2(x_2)\\
\end{bmatrix}
$$

在进行矩阵乘法时，需要注意：**矩阵乘法不满足交换律，但是满足结合律**！

### 矩阵的逆和转置

**单位阵的概念**：$n$ 阶单位阵 $I$ 是一个 $n \times n$ 的，对角线上全为 $1$ ，其余部分为 $0$ 的矩阵，其满足：$A \cdot I = I \cdot A = A$ 。虽然矩阵乘法不满足交换律，但是矩阵和单位阵的乘法是满足交换律的。

**矩阵的逆**：矩阵的逆是一个类似于倒数的概念。只有**方阵**（行数与列数相等的矩阵）才有逆矩阵，矩阵 $A$ 的逆矩阵被记作 $A^{-1}$ 。如果矩阵 $A$ 是方阵且有逆矩阵，那么 $AA^{-1} = A^{-1}A = I$ 。

**矩阵的转置**：将矩阵的行与列交换，成为矩阵的转置。矩阵 $A$ 的转置被记作为 $A^T$。具体地:

$$A^T_{ij} = A_{ji}$$

## 第四章 多元线性回归

### 多特征

假设有 $m$ 个样本，每个样本具有 $n$ 个特征，那么第 $i$ 个样本会被表示为：$A^{(i)}$ ，第 $i$ 个样本的第 $j$ 个特征会被表示为：$A^{(i)}_j$ 。

目标函数被表示为：$h_{\theta}(x) = {\theta}_0 + \sum\limits_{i=1}^n{\theta_i x_i}$ 。

令 $x_0 = 1$ ，那么目标函数变为：$h_{\theta}(x)=\sum\limits_{i=0}^n{\theta_i x_i}$ 。

将参数和特征写成矩阵（列向量）形式，有：$h_{\theta}(x)=\theta^Tx$ 。值得注意的是，此时参数和特征都是 $n+1$ 维的。

### 多元梯度下降法

目标函数：$h_{\theta}(x) = \theta^T x = \sum\limits_{i=0}^n{\theta_i x_i}$ ，其中 $x_0 = 1$ 。

参数：$\theta$（$n+1$ 维向量）

损失函数：$J(\theta) = \dfrac{1}{2m} \sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$ 。

梯度下降更新公式：$\theta_j:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j$ 。

### 特征缩放（归一化）

归一化的目的：让所有的特征都处于相似大小的范围，以让梯度下降得更快。

假设数据都是正数，$\mu$ 为数据的平均值，$s$ 为数据的标准差。

最大值归一化：$x=\dfrac{x}{x_{\max} }$ 。

均值归一化：$x=\dfrac{x - \mu}{x_{\max} - x_{\min} }$ 。

均值方差归一化：$x=\dfrac{x - \mu}{s}$ 。

### 学习率

画出 $J(\theta)$ 随训练轮次变化的曲线可以帮助判断梯度下降是否正常运行。如果 $J(\theta)$ 在某一段递增，通常是因为学习率设置得过大而引起，这个时候可以将学习率调小。

学习率对梯度下降的影响：

- 如果学习率过小，会导致梯度下降收敛得很慢。
- 如果学习率过大，会导致损失函数难以收敛，甚至发散。

在实践中，通常需要尝试多种学习率，找到最适合对应任务的学习率。

### 特征与多项式回归

有的时候可以将特征进行运算，以让线性回归模型拟合更加复杂的函数。

例如房屋的特征有长度和宽度，我们就可以将长度和宽度相乘，得到房屋的面积，作为一个新的特征输入模型，获得更好的效果。

在多项式回归模型中，每一项可以看成是原始特征进行幂运算之后得到的新特征，当然也可以进行开方、取对数等运算。

具体需要使用哪些特征，要如何组合特征，可以由自己自由选择，也可以使用算法去寻找，这也催生了特征工程这一学科。

### 正规方程

正规方程方法是一种计算参数 $\theta$ 最优值的解析方法，它可以一次性计算出参数 $\theta$ 的最优值，而不需要像梯度下降一样进行多次迭代。

假设特征矩阵 $X\in {\mathbb R}^{m\times n + 1}$ ，$m$ 表示有 $m$ 个样本，$n$ 表示有 $n$ 个特征，加一是加了一项 $x_0 = 1$ 。标签矩阵 $y\in {\mathbb R}^{m}$ 。那么最优参数 $\theta$ 可以通过如下公式进行计算：

$$\theta = (X^TX)^{-1}X^Ty$$

其中 $X^T$ 表示矩阵 $X$ 的转置，$(X^TX)^{-1}$ 表示矩阵 $X^TX$ 的逆。数学上可以证明该公式的正确性，在此不进行赘述。

梯度下降法与正规方程法的对比：

- 梯度下降法需要选择学习率，而正规方程法不需要。
- 梯度下降法需要通过多次迭代次啊能求解最优值，而正规方程法不需要迭代。
- 但是梯度下降法在特征数量很多（$n$ 很大）的时候依然有着优秀的表现，而正规方程法由于需要求解一个 $n\times n$ 的矩阵的逆，其时间复杂度为 $\Theta(n^3)$ ，当 $n$ 很大时，效率会非常低下。
- 梯度下降法比正规方程法更加泛用，正规方程法几乎只适用于线性回归。

> 在使用正规方程法时，如果 $X^TX$ 不可逆，应该怎么办？
> 
> 在遇到矩阵不可逆但是要求逆的情况时，可以通过求伪逆实现。
>
> 另外，矩阵不可逆通常是因为以下两种情况：
>
> 1. 不同的特征之间线性相关（特征多余）
> 2. 特征数量太多（例如特征数量大于样本数量）
>
> 要解决这些问题，一般采用删除某些特征，或者正则化的方式。

## 第五章 Octave 教程

人生苦短，我用 Python 。🐶

## 第六章 逻辑回归

### 分类

分类问题是指一类问题，它的输出 $y$ 只有两个离散的取值 $0$ 或 $1$ ，如判断邮件是否为垃圾邮件，网上交易是否属于诈骗，肿瘤是否属于恶性肿瘤等。其中，$y=1$ 的样本被称作为正例；$y=0$ 的样本被称作为负例。一般来说，我们会将希望找到的东西化为正例，但实际上正例与负例的划分是任意的。

线性回归算法不适用于分类问题，理由有如下两个：

1. 线性回归算法对噪声过于敏感。
2. 线性回归算法的输出值可能大于 $1$ 或者小于 $0$ ，这在所有标签都是 $0$ 或 $1$ 的问题中略显奇怪。

因此，本章将引入逻辑回归算法，该算法的输出值会在 $0\sim 1$ 之间。注意：虽然逻辑回归算法中有“回归”二字，但它实际上是一个分类算法。

### 模型表示

定义 $h_\theta(x)$ 表示在参数 $\theta$ 的影响下，给定特征 $x$ ，其属于正例 $y = 1$ 的概率。即：

$$h_\theta(x) = P(y = 1\ |\ x;\theta)$$

$h_\theta(x)$ 通过如下公式进行计算：

$$
\begin{align*}
    h_\theta(x) = g(\theta^T x) \\
    g(z) = \dfrac{1}{1 + e^{-z} }
\end{align*}
$$

这里的 $g(z)$ 被称作为 Sigmoid 函数或者 Logistic 函数（逻辑函数）。其具有如下特点：

1. 值域在 $0$ 到 $1$ 之间：逻辑函数的输出值范围在 $0$ 和 $1$ 之间，当 $x$ 趋近于正无穷大时，$f(x)$ 趋近于 $1$ ；当 $x$ 趋近于负无穷大时，$f(x)$ 趋近于 $0$。
2. 中心对称：逻辑函数在 $x=0$ 处对称，即 $f(0) = 0.5$ 。
3. 平滑性：逻辑函数的曲线平滑，无跳跃点或不连续点。

根据全概率公式，我们可以轻松地通过样本属于正例的概率推出样本属于负例的概率：

$$
\begin{align*}
    P(y = 0\ | \ x;\theta) + P(y = 1\ | \ x;\theta) = 1 \\
    P(y = 0\ | \ x;\theta) = 1 - P(y = 1\ | \ x;\theta)
\end{align*}
$$

### 决策边界

假设当 $h_\theta(x) \ge 0.5$ 时，预测 $y = 1$ ；当 $h_\theta(x) < 0.5$ 时，预测 $y = 0$ 。根据逻辑函数的性质，当 $z \ge 0$ 时，$g(z) \ge 0.5$ ，要使得 $h_\theta(x) \ge 0.5$ ，则要求 $\theta^T x \ge 0$ 。同理，若 $\theta^T x < 0$ ，那么 $h_\theta(x) < 0.5$ ，则会预测 $y = 0$ 。

通过上述分析，我们将 $y$ 的预测转换成了 $\theta^T x$ 值与 $0$ 的大小比较。对于二维的情况， $\theta^T x = 0$ 可以在平面上确定一条直线（曲线），这条直线（曲线）会将平面分成两个部分，其中一个部分中的点会被预测为正例，另外一个部分中的点会被预测为负例。这条直线（曲线）就是模型 $h_\theta(x)$ 的**决策边界**。

值得注意的是：决策边界是模型本身的性质，与数据集无关。数据集仅仅被用来优化模型。

### 损失函数

一般化地，将损失函数写成如下形式：

$$J(\theta) = \dfrac{1}{m}\sum\limits_{i=1}^m {\rm Cost}(h_\theta(x^{(i)}), y^{(i)})$$

对于线性回归，其损失函数为：

$${\rm Cost}(h_\theta(x), y) = \dfrac{1}{2}(h_\theta(x) - y)^2$$

这就是平方差损失函数。但是，对于逻辑回归，如果使用平方差损失函数，会导致损失函数非凸，因此需要另外一个损失函数——对数损失函数：

$$
{\rm Cost}(h_\theta(x), y) = 
\begin{cases}
    -\log(h_\theta(x)), & y = 1 \\
    -\log(1 - h_\theta(x)), & y = 0
\end{cases}
$$

对于 $y = 1$ 的情况，如果输出的 $h_\theta(x) = 1$ ，那么损失函数的值为 $0$ ，而如果输出的 $h_\theta(x) \to 0$ ，那么损失函数的值会趋向于无穷。这对 $y = 0$ 的情况也是类似的。

由于 $y$ 只会为 $0$ 或 $1$ ，因此可以将代价函数简化成如下形式：

$${\rm Cost}(h_\theta(x), y) = -y\log(h_\theta(x)) - (1-y)\log(1 - h_\theta(x))$$

### 梯度下降

通过损失函数对 $\theta$ 求偏导，可以得到梯度下降公式如下：

$$\theta_j := \theta_j - \alpha \sum\limits_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$$

可以发现，这个梯度下降公式与线性回归的下降公式是一致的。但是由于模型的定义发生了变化，所以逻辑回归和线性回归本质上其实是不同的算法。

### 高级优化

除了梯度下降算法，还有一些其他的高级算法可以用来寻找最优的参数值。如共轭梯度、BFGS、L-BFGS 等。这些算法通常不需要手动确定学习率，并且比梯度下降更快。但是其不足之处在于：相较于梯度下降算法，这些算法会有亿点点复杂。

### 多分类问题：一对多

多分类问题是指样本不止两类的情况，即 $y$ 可能为 $2, 3, 4, \cdots$ 。例如判断天气时晴天、多云、下雨或者下雪。

多分类问题的一对多方法的思想是：假设一共有 $n$ 个类别，训练 $n$ 个二分类器，其中第 $i$ 个分类器以第 $i$ 类样本为正例，其他所有样本为负例，输出样本属于第 $i$ 类的概率。即：

$$h^{(i)}_\theta(x) = P(y = i\ |\ x;\theta)\quad (i=1,2,3,\cdots)$$

对于一个新的样本 $x$，将其输入到所有 $n$ 个分类器中，取输出概率最大的那个分类器，作为 $x$ 的所属类别。即：

$$\max\limits_{i}h^{(i)}_\theta(x)$$

## 第七章 正则化

### 过拟合问题

如果特征数量非常庞大，那么模型可能会很好地拟合训练集，但是难以泛化到其他的样本中去，这就是过拟合问题。与过拟合相对，如果模型并不能很好地拟合训练集，就被称作为欠拟合。

高偏差与高方差的概念：

- 偏差（Bias）是指模型对于训练数据的错误偏离程度。当模型具有高偏差时，意味着模型对于训练数据的拟合程度较低，即欠拟合。
- 方差（Variance）是指模型对于不同训练数据集的响应程度。当模型具有高方差时，意味着模型对于训练数据的拟合程度较高，但在处理新的、未见过的数据时表现较差，即过拟合。

为了解决过拟合问题，常常有以下两种方式：

1. **减少特征的数量**：人工选择保留的特征，或者通过模型选择算法选择保留的特征。但是减少特征数量意味着丢失了一些用于解决问题的信息。
2. **正则化**：保留所有的特征，但是让 $\theta$ 的值处于一个很小的范围，这在有许多特征的问题中非常有效，每一个特征都对预测结果产生了一点点影响。

### 损失函数

正则化的思想在于通过在损失函数中加入对参数 $\theta$ 的乘法项，以让 $\theta$ 的值更小，更小的 $\theta$ 值就意味着更“简单”的模型，也意味着更难发生过拟合。

因此，运用了正则化思想的损失函数如下式所示（假设参数数量为 $n$）：

$$J(\theta) = \dfrac{1}{2m}\left[\sum\limits_{i=1}^m (h_\theta(x^{(i)} ) - y^{(i)})^2 + \lambda \sum\limits_{i=1}^n \theta^2_i\right]$$

后一个循环的下标从 $1$ 开始是因为一般不对 $\theta_0$ 进行正则化，但实际上对 $\theta_0$ 进行正则化也问题不大。

公式中的 $\lambda$ 是正则化系数，用于控制模型损失和正则化损失的权重，以引导学习算法更关注数据的拟合还是模型的简化。如果 $\lambda$ 被设置得过大，则可能导致以下后果：

- 算法设计得过于完美，把 $\lambda$ 设置得很大并不会造成什么影响（一般来说是想 peach）。
- 算法不再能防止过拟合。
- 算法甚至可能导致欠拟合。
- 梯度可能很难下降。

### 线性回归中的正则化

线性回归的求解有梯度下降和正规方程两种解法，因此对这两种情况分别进行讨论。

对于梯度下降法，损失函数为：

$$J(\theta) = \dfrac{1}{2m}\left[\sum\limits_{i=1}^m (h_\theta(x^{(i)} ) - y^{(i)})^2 + \lambda \sum\limits_{i=1}^n \theta^2_i\right]$$

因此梯度下降公式为：

$$
\begin{align*}
\theta_0 &:= \theta_0 - \alpha \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_0 \\
\theta_j &:= \theta_j - \alpha\left[ \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j - \dfrac{\lambda}{m}\theta_j \right]
\end{align*}
$$

注意，分情况讨论的原因是一般不对 $\theta_0$ 进行正则化，同时对下面的公式稍作变形，可以得到：

$$\theta_j := \theta_j(1 - \alpha\dfrac{\lambda}{m}) - \alpha \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$$

由于一般 $\alpha$ 很小，而 $m$ 很大，因此 $1 - \alpha\dfrac{\lambda}{m}$ 会很接近于 $1$ 。于是我们就可以看到加入正则化之后的梯度下降相当于每一次先稍微减小一下 $\theta$ 的值，再进行常规的梯度下降，这与正则化想要减小参数值大小的思想是一致的。

对于正规方程法，最优 $\theta$ 求解公式为：

$$
\theta = \left(X^TX + \lambda
\begin{bmatrix}
    0 & \\
      & 1 \\
      &   & 1 \\
      &   &   & \ddots \\
      &   &   &   & 1
\end{bmatrix}
\right)^{-1}X^Ty$$

值得注意的是，在这个公式中，矩阵 $X^TX + \lambda\begin{bmatrix}0 & \\& 1 \\&   & 1 \\&   &   & \ddots \\&   &   &   & 1\end{bmatrix}$ 一定是可逆的。

### 逻辑回归中的正则化

在逻辑回归中，正则化的损失函数为：

$$J(\theta) = -\left[\sum\limits_{i=1}^m y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1 - h_\theta(x^{(i)}))\right] + \dfrac{\lambda}{2m}\sum\limits_{i=1}^n\theta^2_j$$

其梯度下降更新公式为：

$$
\begin{align*}
\theta_0 &:= \theta_0 - \alpha \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_0 \\
\theta_j &:= \theta_j - \alpha\left[ \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j - \dfrac{\lambda}{m}\theta_j \right]
\end{align*}
$$

可以看到，正则化的逻辑回归，其梯度下降公式与正则化线性回归的梯度下降公式是一样的。但是同样的：因为两种问题的模型在定义上就是不同的，所以本质上，这两种梯度下降是不一样的。

## 第八章 神经网络

### 非线性模型

在实际的机器学习问题中，特征的数量往往非常庞大，这导致我们需要非线性模型来解决问题。

假设原始特征数量为 $n$ ，如果使用线性回归或逻辑回归算法，即使仅仅包含了所有的一次项特征和二次项特征，那么模型所考虑的特征数量是 $\Theta(n^2)$ 级别的。大量的特征会导致模型过于复杂，而容易导致过拟合。而如果舍弃一些特征，就丢失了很多用于解决问题的信息。

这个时候，就需要引入新的算法，而**神经网络**在学习复杂的非线性模型上被证明是一种好得多的算法。

### 神经元和大脑

神经系统科学家曾经做过一个有趣的实验：把耳朵到听觉皮层的神经切断，而将眼睛向大脑输入信号的神经接到听觉皮层上，这样视觉信号就会被传输到听觉皮层中。实验发现：在这样的情况下，听觉皮层能够学会“看”。这个实验被称作为神经重接实验。

神经重接实验告诉我们：大脑可以进行多种复杂的任务不是因为大脑中有各种各样不同的“程序”，而是因为大脑有一套学习算法，大脑通过这个算法可以学会各种各样数据的处理。因此，通过寻找一个类似于大脑学习算法的算法，将其运用在计算机上，我们的计算机就很大概率能够变得更加智能。这也导致了神经网络模型的出现。

其他的一些关于大脑学习能力的例子：

- BrainPort 系统：在前额上戴上一个灰度摄像头，将灰度信号用一根线连接到舌头上安装的电极阵列上。通过这样的方式，可以让我们的舌头学会“看”东西。
- 人体回声定位：通过特殊的培训，可以让人们打响指，或者咂舌头，并学会解读从环境中反射回来的声波，从而避免撞到障碍物。
- 触觉皮带：将其戴在腰上，并将皮带上的蜂鸣器打开，那么只有朝北的那个蜂鸣器会发出声音，这可以让人拥有像鸟类一样的感知方向的能力。
- 如果在青蛙身上移植第三只眼睛，那么青蛙能够学会使用那只眼睛。

### 模型表示

大脑内的一个神经元会有若干树突用于接收信息，以及一个轴突用于传出信息。根据这样的结构，我们可以构造出如下的逻辑单元，我们一般将其称为神经元。

![神经元](/images/MLofAndrew/Logistic_unit.png)

这个神经元接收输入 $x_1, x_2, x_3$ ，对其进行一些计算，最后输出 $h_\theta(x)$ 。这里 $h_\theta(x) = \dfrac{1}{1 + e^{-\theta^Tx} }$ ，其中 $x=\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ x_3\end{bmatrix}$ 是输入数据，$\theta=\begin{bmatrix}\theta_0 \\ \theta_1 \\ \theta_2 \\ \theta_3\end{bmatrix}$ 是可训练参数。在实际应用过程中，一般还会输入偏置项 $x_0$ ，在图中为了方便，就未画出。

将若干个神经元连接起来，就能够得到一个神经网络：

![神经网络](/images/MLofAndrew/Neural_Network.png)

为了表述方便，将这个网络从左至右依次称为第 $1$、$2$、$3$ 层。用 $a^{(j)}_i$ 表示第 $j$ 层的第 $i$ 个神经元的输出值，$\theta^{(j)}$ 表示第 $j + 1$ 层的神经元处理第 $j$ 层神经元传过来的信号时的参数。

输入层、输出层、隐藏层的定义：

- **输入层**：一般将最左边的层称为输入层，因为这一层的输入是原始特征 $x$ ，其只是起到一个传递 $x$ 的作用，因此其输出值也是 $x$ ，即可以认为：$a^{(0)} = x$ 。
- **输出层**：一般将最右边的层称为输出层，因为这一层用于输出最终结果，其输出对应标签 $y$ 。
- **隐藏层**：夹在输入层和输出层之间的层被称为隐藏层，隐藏层用于提取特征，中间层的输出不会在数据集中体现。

神经网络的计算方式如下式所示（假设前一层一共有 $n$ 个特征）：

$$a^{(j)}_i = g(\sum\limits_{k=0}^n\theta^{(j-1)}_{j-1,k}a^{(j-1)}_k)$$

表示为向量形式：

$$a^{(j)} = g(\theta^{(j - 1)}a^{(j - 1)})$$

这里的 $g$ 代表 Sigmoid 函数，即 $g(x) = \dfrac{1}{1 + e^{-x} }$ 。在上述的例子中，最终输出值 $h_\theta(x) = a^{(3)}_1$ 。

观察计算方式可以发现：如果第 $j$ 层一共有 $s_j$ 个神经元，第 $j + 1$ 层一共有 $s_{j + 1}$ 个神经元，那么参数 $\theta^{(j)}$ 的形状为：$s_{j + 1}\times (s_j + 1)$ 。

特别地，输出层的计算方式如下：

$$h_\theta(x) = a^{(3)} = g(\theta^{(2)}a^{(2)})$$

观察输出层的计算公式可以发现，输出层相当于一个逻辑回归层，但是相较于传统的逻辑回归，输出层的逻辑回归使用的特征是隐藏层提取出来的高级特征，而不是原始特征，多层线性函数和激活函数的嵌套，给予了神经网络算法强大的拟合能力。

举个例子：可以很容易构造出进行与、或、非运算的神经网络。通过对这三种神经网络进行嵌套，可以构造出进行异或运算的神经网络。这样我们就通过线性运算和激活函数，拟合了一个复杂函数。具体嵌套方式如下式所示：

$$x_1\ {\rm XNOR}\ x_2 =(x_1\ {\rm AND}\ x_2)\ {\rm OR}\ (({\rm NOT}\ x_1)\ {\rm AND}\ ({\rm NOT}\ x_2))$$

### 多分类

使用神经网络解决多分类问题本质上是逻辑回归一对多方法的应用。具体地，假设一共有 $4$ 个类别，那么就让输出层一共有 $4$ 个神经元，每个神经元分别输出样本属于对应类别的概率。理想情况下，当样本属于第一类时，神经网络的输出为：$h_\theta(x)\approx \begin{bmatrix}1\\0\\0\\0\end{bmatrix}$ ；当样本属于第二类时，神经网络的输出为：$h_\theta(x)\approx \begin{bmatrix}0\\1\\0\\0\end{bmatrix}$；依此类推……

对于这样的算法，我们所构建的数据集应当满足如下形式：$(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(m)},y^{(m)})$ ，其中 $y^{(i)}$ 是 $\begin{bmatrix}1\\0\\0\\0\end{bmatrix}$、$\begin{bmatrix}0\\1\\0\\0\end{bmatrix}$、$\begin{bmatrix}0\\0\\1\\0\end{bmatrix}$、$\begin{bmatrix}0\\0\\0\\1\end{bmatrix}$ 四者之一。

## 第九章 神经网络的学习

### 损失函数

假设神经网络一共有 $L$ 层，第 $l$ 层的神经元个数（不包括偏置项）为 $s_l$ 。样本的类别一共有 $K$ 个，特别地，对于二分类问题，$K=1$ ，因为二分类问题只需要使用一个分类器即可。

于是可以得到：$h_\theta(x) \in {\mathbb R}^K$ ，令 $(h_\theta(x))_i$ 表示神经网络的第 $i$ 个输出。那么神经网络的损失函数如下式所示：

$$J(\theta) = -\dfrac{1}{m}\left[\sum\limits_{i=1}^m\sum\limits_{k=1}^K y^{(i)}\log(h_\theta(x^{(i)})_k) + (1 -y^{(i)})\log(1 - h_\theta(x^{(i)})_k)\right] + \dfrac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1} }(\theta^{(l)}_{ji})^2$$

公式的前半段是用于拟合数据的损失，后半段是用于正则化的损失。

### 反向传播

定义  $\delta^{(l)}_j$ 表示第 $l$ 层的第 $j$ 个神经元的“误差”。对于一个四层的神经网络，其 $\delta$ 计算方式如下：

$$
\begin{align*}
    \delta^{(4)} &= a^{(4)} - y \\
    \delta^{(3)} &= (\theta^{(3)})^T\delta^{(4)} \cdot g^{'}(\theta^{3}a^{(3)}) \\
    \delta^{(2)} &= (\theta^{(2)})^T\delta^{(3)} \cdot g^{'}(\theta^{2}a^{(2)}) 
\end{align*}
$$

这里的 $g^{'}(x)$ 是 Sigmoid 函数在 $x$ 处的导数。注意，我们并不需要计算 $\delta^{(1)}$ ，因为第一层（输入层）的神经元只是简单地把输入传递给下一层，而不涉及到什么计算。

通过上述计算，我们就可以用如下公式计算每一个参数的梯度：

$$\dfrac{\partial J}{\partial  \theta^{(l)}_{ij} } = a^{(l)}_j\delta^{l + 1}_i$$

写成向量化的形式为：

$$\dfrac{\partial J}{\partial  \theta^{(l)} } = \delta^{l + 1}(a^{(l)})^T$$

具体证明过程可以参考 [jumper17 的博客](https://blog.csdn.net/jumpe_17/article/details/119682594) ，形象化理解可以参考[李宏毅老师的视频](https://www.bilibili.com/video/BV1Wv411h7kN/?p=14)。

> 更深刻的理解：
> 
> 根据链式求导法则：$\dfrac{\partial J}{\partial \theta} = \dfrac{\partial J}{\partial \theta a}\dfrac{\partial \theta a}{\partial \theta}$ 。而 $\dfrac{\partial \theta a}{\partial \theta} = a$ 。所以，实际上可以将 $\delta^{(l)}_j$ 视作损失函数 ${\rm Cost}(i)$ 对 $\theta^{(l-1)}_ja^{(l-1)}$ 的导数，即：
> $$\delta^{(l)}_j = \dfrac{\partial }{\partial(\theta^{(l-1)}_ja^{(l-1)})}{\rm Cost}(i)$$
> 而根据链式求导法则，$\delta^{(l)}_j$ 又可以根据下一层的 $\delta^{(l+1)}$ 计算出来。这是一个递归的过程，可以从后往前将递归转换为递推，就是反向传播算法了。

对于 batch 训练，可以对于每个样本算出其 $\delta$ ，再累加求和。即样本的总体“误差” $\Delta$ 按如下公式计算：

$$\Delta^{(l)} := \sum\delta^{l + 1}(a^{(l)})^T$$

最后每个参数的导数为：

$$
\dfrac{\partial J(\theta)}{\partial \theta^{l}_{ij} } = 
\begin{cases}
    \dfrac{1}{m} \Delta^{l}_{ij} + \dfrac{\lambda}{m}\theta^{(l)}_{ij}, &j\neq 0 \\
    \\
    \dfrac{1}{m} \Delta^{l}_{ij}, &j=0
\end{cases}
$$

### 梯度检验

在实现反向传播算法的过程中，可能会出现一些 bug ，这时候就需要通过梯度检验来确保反向传播算法正确计算梯度。

一般使用双侧差分的方式进行梯度检验，即：

$$\dfrac{ {\rm d} }{ {\rm d}\theta}J(\theta) \approx \dfrac{J(\theta + \epsilon) - J(\theta - \epsilon) }{2\epsilon}$$

这里 $\epsilon$ 是一个很小很小的值，如 $10^{-4}$ 。双侧差分意味着在 $\theta$ 的两边都增加一个 $\epsilon$ ，单侧差分则只会在某一侧增加，一般来说，双侧差分更加准确。

如果 $\theta$ 是一个向量，那么就对每一个分量求偏导。具体地，假设 $\theta = [\theta_1, \theta_2, \cdots, \theta_n]$ ，那么分别计算：

$$
\begin{align*}
    \dfrac{\partial}{\partial \theta_1}J(\theta) &\approx \dfrac{J(\theta_1 + \epsilon, \theta_2, \cdots, \theta_n)-J(\theta_1 - \epsilon, \theta_2, \cdots, \theta_n)}{2\epsilon} \\
    \dfrac{\partial}{\partial \theta_2}J(\theta) &\approx \dfrac{J(\theta_1, \theta_2 + \epsilon, \cdots, \theta_n)-J(\theta_1, \theta_2 - \epsilon, \cdots, \theta_n)}{2\epsilon} \\
    \vdots \\
    \dfrac{\partial}{\partial \theta_n}J(\theta) &\approx \dfrac{J(\theta_1, \theta_2, \cdots, \theta_n + \epsilon)-J(\theta_1, \theta_2, \cdots, \theta_n - \epsilon)}{2\epsilon} \\
\end{align*}
$$

最后，在训练时一定不要执行梯度检验，因为梯度检验的效率很低。在训练时应当使用反向传播算法。

### 随机初始化

在开始训练之前，需要为网络中的参数指定初始值。如果将所有的参数都设置为同一个值（比如 $0$），那么同一层的神经元的输出都会相同，反向传播的梯度也会相同，这导致同一层的所有神经元学习到的特征是相同的，从而极大地减弱了神经网络的学习能力。因此，在进行参数初始化时，要利用随机初始化将参数 $\theta$ 随机到 $[-\epsilon, \epsilon]$ 的范围内，即 $-\epsilon \le \theta \le \epsilon$ 。一个可行的初始化方式为：

$$\theta = {\rm Random}(0, 1) \times 2\epsilon - \epsilon$$

这里的 ${\rm Random}(0, 1)$ 是指 $[0, 1]$ 之间的随机数。

### 利用神经网络解决问题的一般步骤

首先需要确定网络架构（输入层、隐藏层、输出层及其之间的连接方式）。其中输入层神经元数量和特征数量相同，输出层神经元数量和类别总数相同（对于二分类问题可以只有一个输出）。对于隐藏层：一般默认只有一层隐藏层，如果有多层隐藏层，那么不同层的神经元数量应该相同。当然，在深度神经网络越来越流行的情况下，这个默认的准则已经效果越来越小了。

确定好网络架构之后，就是训练神经网络了。训练神经网络一般遵循以下六个步骤：

1. 初始化神经网络参数。
2. 实现前向传播方法计算 $h_\theta(x)$ 。
3. 实现损失函数 $J(\theta)$ 的计算。
4. 实现反向传播算法计算偏导数 $\dfrac{\partial}{\partial\theta^{(l)}_{jk} }J(\theta)$ 。
5. 进行梯度检验，以确保反向传播梯度计算的正确性。
6. 通过反向传播算法进行梯度下降或者使用其他高级优化算法优化网络参数。

## 第十章 机器学习算法应用技巧

### 模型评估

为了评估模型，通常将数据集划分成训练集和测试集，先在训练集上训练模型，再在测试集上计算测试集误差。度量误差的方式可以是损失函数，或者准确率。

### 模型选择（为什么需要验证集）

有的时候我们并不清除要用怎样一个模型（超参数）以拟合数据。因此在训练与测试之后，需要进行参数调优。如果我们使用训练集训练模型，再用测试集计算测试集误差，以该误差作为判断模型泛化能力的标准。那么在参数调优的过程中，相当于把测试集当作训练集，用于拟合超参数。这样得到的超参数是不能很好地表现模型的泛化能力的。因此一般要把数据集分为三部分：训练集、验证集、测试集。训练集用于训练模型，验证集用于参数调优，测试集用于评估模型泛化能力。

### 偏差、方差和正则化

当模型效果不好时，往往是因为其面临着高偏差（欠拟合）或者高方差（过拟合）的问题。这两种问题分别有以下特征：

- 高偏差（欠拟合）表现为训练集误差和验证集误差都很高。
- 高方差（过拟合）表现为训练集误差很低，但是验证集误差很高。

一般会使用正则化来缓解过拟合问题。当使用正则化时，训练的损失函数为：模型误差 $+$ 正则化项。**但是在评估训练集误差和验证集误差时，一般会将正则化项删除**。正则化系数 $\lambda$ 对误差的影响如下：

- 当 $\lambda$ 较小时，相当于模型没有正则化，此时模型会倾向于过拟合，即训练集误差很低，但是验证集误差很高。
- 当 $\lambda$ 较大时，相当于正则化过度，此时模型会倾向于欠拟合，即训练集误差和验证集误差都很高。

### 学习曲线

在训练的过程中，人为控制训练集的大小，画出训练集误差和验证集误差随着训练集大小变化的曲线，有助于判断模型处于欠拟合状态还是过拟合状态。

一般来说，随着训练集的增大，训练集误差会不断增大，而验证集误差会不断减小，最后收敛到相似的适中的值。

- 当模拟欠拟合时，训练集误差和验证集误差都会收敛到很高的值，并且在一定数据集大小之后，即使数据集大小再增加，验证集误差也不会减小太多（几乎呈一条水平直线）。因此增加数据量并不能解决欠拟合问题。
- 当模型过拟合时，训练集误差会维持在一个较低的水平，而验证集误差会维持在一个较高的水平，两者差距较大。但是随着训练集大小的增加，训练集误差总是会上升，验证集误差总是会下降，最后收敛到相似的合适值。因此增加数据量有助于缓解过拟合问题。

### 过拟合（欠拟合）问题的一些解决方法

- 解决过拟合问题的方法：增加训练集大小，使用更少的特征，调大正则化参数 $\lambda$ ……
- 解决欠拟合问题的方法：使用更多的特征、增加多项式特征、调小正则化参数 $\lambda$ ……

另外，使用更小的神经网络会有更少的参数，更容易欠拟合，但是计算起来速度很快；使用更大的神经网络会有更多的参数，更容易过拟合，而且计算起来速度偏慢。但是总体上来说，大的神经网络性能要优于小的神经网络。

## 第十一章 机器学习系统设计

### 误差分析

当我们设计一个机器学习系统时，推荐的实现步骤如下：

1. 首先实现一个很简单的算法，并再验证集上进行测试。
2. 画出学习曲线，以确定是否需要更多的数据或者特征。
3. 进行误差分析，人工检查算法分类出错的样本，以找出算法出错的趋势。如分析哪一类的样本经常被误分类，被误分类的样本会有什么样的特征。

同时，可量化的评价指标也很重要。通过引入可量化的评价指标，可以很直观地评价两种算法的优劣，从而方便尝试各种想法。

### 非对称性分类的评价指标

在一些正类和负类样本数量差距非常大的环境（如只有 $0.5\%$ 的人患有癌症）下，分类准确率不一定能很好反应处算法的性能。比如一个神经网络算法拥有 $99\%$ 的准确率，但是如果将所有人预测为没有癌症，这个策略的准确率为 $99.5\%$ 。后者在指标上比前者更优，但是显然不合里。因此需要更加准确的指标。一些常用的指标有精确率和召回率：

- **精确率（Precision）**：表示在所有预测为正类的样本中，真正为正类的样本的比例。
- **召回率（Recall）**：表示在所有真正为正类的样本中，预测为正类的样本的比例。

在非对称性分类中，一般将样本较少的类视为正类。

通俗地讲，精确率是判断模型预测准不准的指标，又被称为查准率；召回率是判断模型预测全不全的指标，又被称为查全率。

### 精确率和召回率的权衡

一般来讲，较高的精确率会带来较低的召回率；较高的召回率会带来较低的精确率（例如二分类问题中，阈值的设置）。因此需要权衡精确率和召回率带来的影响，以判断模型的表现。有以下两种计算方式：

1. **取平均**：$\dfrac{ {\rm Precision} + {\rm Recall} }{2}$ ，这个方法以精确率和召回率的算数平均值作为最终指标，其实并不好，因为算术平均值受极端值影响太大。
2. **$F_1$ 分数**：$2\dfrac{ {\rm Precision} \times {\rm Recall} }{ {\rm Precision} + {\rm Recall} }$ ，这个方法以精确率和召回率的调和平均值作为最终指标。调和平均值受到较小值的影响更大，因为小数的倒数较大，对整体平均值的贡献更大，最终值也会更低。调和平均值考虑到了数值之间的相对关系，是更加合理的评价指标。

### 机器学习中的数据

在保证数据中的特征 $x$ 提供了足够多的信息以预测 $y$ 的情况下，增大数据量能够有效地提升机器学习算法的性能。一个判断信息是否足够的有效方法是判断给定相应的 $x$ ，人类专家能否判断出 $y$ 。

模型与数据的关系：

- 模型越复杂，偏差越低，越不容易欠拟合。
- 数据越多，方差越低，越不容易过拟合。
