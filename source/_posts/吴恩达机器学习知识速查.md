---
title: 吴恩达机器学习知识速查
author: lornd
mathjax: true
date: 2023-02-19 15:52:09
url: MLofAndrew
tags:
pin: true
---

## 第一章 绪论

### 机器学习

Arthur Samuel 的定义：在不被明确定义的情况下，给予计算机学习的能力的研究领域。

Tom Mitchell 的定义：计算机从经验 $E$ 中学习，解决某一任务 $T$ ，进行性能度量 $P$ 。通过 $P$ 评测其在任务 $T$ 上的表现，这个表现会因为 $E$ 而提高。

常见的机器学习算法：监督学习、无监督学习。

### 监督学习

监督学习给予机器学习算法一个包含“正确答案”的数据集，即训练集中含有标签（label）。

监督学习可以被分为两类问题：

- 回归问题：预测连续值，如预测房价。
- 分类问题：预测离散值，如预测肿瘤是否良性。

### 无监督学习

无监督学习仅仅给予机器学习算法一个数据集，不包含标签，要求算法从数据中发掘数据的结构。这种问题被称作为聚类问题。

无监督学习的常见应用：

- 新闻聚类：将讨论同一件事新闻放到一起。
- 基因工程：判断某一基因在不同的人身上表达的程度。
- 组织计算集群：通过聚类算法判断哪些机器趋向于协同工作，将这些机器放在一起有助于节省资源。
- 社交网络分析：识别同一个圈子中的朋友，并且判断哪些人互相认识。
- 市场细分：将公司客户划分到不同的细分市场，以进行更加精准的推荐。
- 天文分析：帮助探索星系的形成。
- 音频分离：鸡尾酒聚会问题。

## 第二章

待补充

## 第三章

待补充

## 第四章 多元线性回归

### 多特征

假设有 $m$ 个样本，每个样本具有 $n$ 个特征，那么第 $i$ 个样本会被表示为：$A^{(i)}$ ，第 $i$ 个样本的第 $j$ 个特征会被表示为：$A^{(i)}_j$ 。

目标函数被表示为：$h_{\theta}(x) = {\theta}_0 + \sum\limits_{i=1}^n{\theta_i x_i}$ 。

令 $x_0 = 1$ ，那么目标函数变为：$h_{\theta}(x)=\sum\limits_{i=0}^n{\theta_i x_i}$ 。

将参数和特征写成矩阵（列向量）形式，有：$h_{\theta}(x)=\theta^Tx$ 。值得注意的是，此时参数和特征都是 $n+1$ 维的。

### 多元梯度下降法

目标函数：$h_{\theta}(x) = \theta^T x = \sum\limits_{i=0}^n{\theta_i x_i}$ 。

参数：$\theta$（$n+1$ 维向量）

损失函数：$J(\theta) = \dfrac{1}{2m} \sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$ 。

梯度下降更新公式：$\theta_j:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j$ 。

### 特征缩放（归一化）

归一化的目的：让梯度下降得更快。

假设数据都是正数，$\mu$ 为数据的平均值，$s$ 为数据的标准差。

最大值归一化：$x=\dfrac{x}{x_{\max}}$ 。

均值归一化：$x=\dfrac{x - \mu}{x_{\max}}$ 。

均值方差归一化：$x=\dfrac{x - \mu}{s}$ 。

### 学习率

画出 $J(\theta)$ 随训练轮次变化的曲线可以帮助判断梯度下降是否正常运行。如果 $J(\theta)$ 在某一段递增，通常是因为学习率设置得过大而引起，这个时候可以将学习率调小。

学习率对梯度下降的影响：

- 如果学习率过小，会导致梯度下降收敛得很慢。
- 如果学习率过大，会导致损失函数难以收敛，甚至发散。

在实践中，通常需要尝试多种学习率，找到最适合对应任务的学习率。
