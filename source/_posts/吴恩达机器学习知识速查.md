---
title: 吴恩达机器学习知识速查
author: lornd
mathjax: true
date: 2023-02-19 15:52:09
url: MLofAndrew
tags:
pin: true
---

## 第一章 绪论

### 机器学习

Arthur Samuel 的定义：在不被明确定义的情况下，给予计算机学习的能力的研究领域。

Tom Mitchell 的定义：计算机从经验 $E$ 中学习，解决某一任务 $T$ ，进行性能度量 $P$ 。通过 $P$ 评测其在任务 $T$ 上的表现，这个表现会因为 $E$ 而提高。

常见的机器学习算法：监督学习、无监督学习。

### 监督学习

监督学习给予机器学习算法一个包含“正确答案”的数据集，即训练集中含有标签（label）。

监督学习可以被分为两类问题：

- **回归问题**：预测连续值，如预测房价。
- **分类问题**：预测离散值，如预测肿瘤是否良性。

### 无监督学习

无监督学习仅仅给予机器学习算法一个数据集，不包含标签，要求算法从数据中发掘数据的结构。这种问题被称作为聚类问题。

无监督学习的常见应用：

- **新闻聚类**：将讨论同一件事新闻放到一起。
- **基因工程**：判断某一基因在不同的人身上表达的程度。
- **计算集群组织**：通过聚类算法判断哪些机器趋向于协同工作，将这些机器放在一起有助于节省资源。
- **社交网络分析**：识别同一个圈子中的朋友，并且判断哪些人互相认识。
- **市场细分**：将公司客户划分到不同的细分市场，以进行更加精准的推荐。
- **天文分析**：帮助探索星系的形成。
- **音频分离**：鸡尾酒聚会问题。

## 第二章 单元线性回归

### 模型表示

在训练集中，定义一些符号：

- $m$：训练集的大小。
- $x$：输入变量 / 特征。
- $y$：输出变量 / 标签。

基于上述符号，可以使用 $(x, y)$ 来表示一个训练样本。具体地，使用上标 $(x^{(i)}, y^{(i)})$ 来表示第 $i$ 个训练样本。

学习算法的目的是根据给定的数据集，找到一个假设函数 $h$ ，其输入为 $x$ ，输出为 $y$ 。在单元线性回归中，假设 $y$ 与 $x$ 呈线性关系，即 $h_{\theta}(x) = \theta_0 + \theta_1 x$ ，其中 $h_{\theta}(x)$ 表示 $h$ 是关于 $x$ 的函数，可训练参数为 $\theta$ ，有的时候，$h_{\theta}(x)$ 也会被简写成 $h(x)$ 。

### 损失函数

给定训练集，要求找到一个合理的 $h$ ，因此如何确定 $\theta$ 的值就成为了一个重要的问题。一个直观的想法是：选择对于每个训练样本 $(x, y)$ 都能让 $h(x)$ 与 $y$ 接近的 $\theta$ 。因此通过需要一个函数 $J(\theta)$ 来度量 $h(x)$ 与 $y$ 的接近程度。

$$J(\theta) = \dfrac{1}{2m}\sum\limits_{i=1}^m \left(h_{\theta}(x^{(i)}) - y^{(i)}\right)^2$$

其中平方是为了将负数的距离变成正数，$\dfrac{1}{m}$ 是对所有训练样本取平均，$\dfrac{1}{2}$ 是为了方便求导。这个函数被称作为**损失函数** 。而训练的目标就是最小化损失函数，即：

$$\underset{\theta} { {\rm minimize } }\ J(\theta)$$

### 梯度下降

现在我们拥有一个函数 $J(\theta_0, \theta_1)$ ，我们要求其最小值以及对应的 $\theta$ 值。在没有求最值公式的情况下，一个暴力的方法是：先随机指定 $\theta_0$ 和 $\theta_1$ 的值，接着不断调整它们的值，使得 $J(\theta_0, \theta_1)$ 变小，最后 $J(\theta)$ 会收敛到一个局部最小值。

如何使 $J(\theta)$ 下降的速度最快，那就是往梯度的反方向更新参数了。具体地，重复以下步骤直至收敛：

$$\theta_i := \theta_i - \alpha \dfrac{\partial}{\partial \theta_i}J(\theta), (i = 0, 1, \dots)$$

其中 $\alpha$ 是学习率，用于控制参数更新的幅度和下降的速率。对于梯度下降算法，有两点值得注意：

1. 不同参数需要同步更新，以保证所有参数是根据同一个梯度进行更新。
2. 不同的初始值可能导致最后收敛到不同的局部最优值。

学习率对梯度下降的影响：

- 如果学习率过小，会导致梯度下降收敛得很慢。
- 如果学习率过大，会导致损失函数难以收敛，甚至发散。

这里所说的梯度下降算法也被称作为 Batch 梯度下降，Batch 是指每一步梯度下降都遍历了整个训练集。

## 第三章 线性代数

### 矩阵和向量

矩阵是由数字组成的二维数组，其维度被定义为行数 $\times$ 列数。对于矩阵 $A$ ，用 $A_{i, j}$ 表示矩阵 $A$ 中第 $i$ 行第 $j$ 列的元素。

向量是一个 $n\times 1$ 的矩阵，其维度就是 $n$ 维。对于向量 $y$ ，用 $y_i$ 表示向量 $y$ 中的第 $i$ 个元素。

### 矩阵的加法与数乘

只有两个同型矩阵（维度相同）才能相加。两个 $n\times m$ 的矩阵相加，得到的结果是每个元素对应相加。即若 $C = A + B$ ，那么 $C_{i, j} = A_{i, j} + B_{i, j}$ 。

矩阵的数乘，其结果是矩阵中的每一个元素与标量相乘。即若 $B=\alpha A$ ，那么 $B_{i, j} = \alpha A_{i, j}$ 。

矩阵的加法和数乘均不改变矩阵的形状。

### 矩阵向量乘法

一个矩阵左乘一个向量，向量的维数必须与矩阵的列数相等。即一个 $n\times m$ 的矩阵只能与一个 $m$ 维的向量相乘。结果为一个 $n$ 维向量，其第 $i$ 个元素矩阵的第 $i$ 行与向量逐元素相乘再相加的结果，即如果矩阵 $A$ 与向量 $y$ 相乘得到了向量 $z$ ，那么：

$$z_i = \sum\limits_{j = 1}^{m} A_{i, j}y_j$$

可以将矩阵向量乘法的思想运用到线性函数的计算中，假设 $x_1, x_2, x_3, x_4$ 是四个输入，线性函数是 $h(x) = \theta_0 + \theta_1 x$ ，那么有：

$$
\begin{bmatrix}
    1 & x_1 \\
    1 & x_2 \\
    1 & x_3 \\
    1 & x_4
\end{bmatrix}
\times
\begin{bmatrix}
    \theta_0 \\
    \theta_1 \\
\end{bmatrix}
=
\begin{bmatrix}
    h(x_1) \\
    h(x_2) \\
    h(x_3) \\
    h(x_4)
\end{bmatrix}
$$

通过这样的方式，我们可以将多次线性运算转换为一次矩阵运算。具体可以表示为：

$$\rm Prediction = DataMatrix \times Parameters$$

### 矩阵乘法

两个矩阵相乘，后一个矩阵的行数必须与前一个矩阵的列数相同，得到的矩阵的维度维：前一个矩阵的行数 $\times$ 后一个矩阵的列数。这意味着，一个 $n\times m$ 的矩阵与一个 $m\times o$ 的矩阵相乘，结果为一个 $n\times o$ 的矩阵。

矩阵乘法的计算，可以看成多个矩阵与向量乘法计算的拼接。具体地，将 $m\times o$ 的矩阵的第 $i$ 列提取出来，与 $n\times m$ 的矩阵进行乘法运算，就得到了结果矩阵的第 $i$ 列。将全部 $o$ 列计算完毕之后，拼接起来就得到了最终的计算结果。

同矩阵向量乘法相同。矩阵之间的乘法可以对数据矩阵同时进行多种线性运算，假设有两个输入 $x_1, x_2$ ，两个线性运算 $h_1(x), h_2(x)$，那么有：

$$
\begin{bmatrix}
    1 & x_1 \\
    1 & x_2 \\
\end{bmatrix}
\times
\begin{bmatrix}
    h_1 & h_2
\end{bmatrix}
=
\begin{bmatrix}
    h_1(x_1) & h_2(x_1) \\
    h_1(x_2) & h_2(x_2)\\
\end{bmatrix}
$$

在进行矩阵乘法时，需要注意：**矩阵乘法不满足交换律，但是满足结合律**！

### 矩阵的逆和转置

**单位阵的概念**：$n$ 阶单位阵 $I$ 是一个 $n \times n$ 的，对角线上全为 $1$ ，其余部分为 $0$ 的矩阵，其满足：$A \cdot I = I \cdot A = A$ 。虽然矩阵乘法不满足交换律，但是矩阵和单位阵的乘法是满足交换律的。

**矩阵的逆**：矩阵的逆是一个类似于倒数的概念。只有**方阵**（行数与列数相等的矩阵）才有逆矩阵，矩阵 $A$ 的逆矩阵被记作 $A^{-1}$ 。如果矩阵 $A$ 是方阵且有逆矩阵，那么 $AA^{-1} = A^{-1}A = I$ 。

**矩阵的转置**：将矩阵的行与列交换，成为矩阵的转置。矩阵 $A$ 的转置被记作为 $A^T$。具体地:

$$A^T_{ij} = A_{ji}$$

## 第四章 多元线性回归

### 多特征

假设有 $m$ 个样本，每个样本具有 $n$ 个特征，那么第 $i$ 个样本会被表示为：$A^{(i)}$ ，第 $i$ 个样本的第 $j$ 个特征会被表示为：$A^{(i)}_j$ 。

目标函数被表示为：$h_{\theta}(x) = {\theta}_0 + \sum\limits_{i=1}^n{\theta_i x_i}$ 。

令 $x_0 = 1$ ，那么目标函数变为：$h_{\theta}(x)=\sum\limits_{i=0}^n{\theta_i x_i}$ 。

将参数和特征写成矩阵（列向量）形式，有：$h_{\theta}(x)=\theta^Tx$ 。值得注意的是，此时参数和特征都是 $n+1$ 维的。

### 多元梯度下降法

目标函数：$h_{\theta}(x) = \theta^T x = \sum\limits_{i=0}^n{\theta_i x_i}$ ，其中 $x_0 = 1$ 。

参数：$\theta$（$n+1$ 维向量）

损失函数：$J(\theta) = \dfrac{1}{2m} \sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$ 。

梯度下降更新公式：$\theta_j:=\theta_j-\alpha\dfrac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j$ 。

### 特征缩放（归一化）

归一化的目的：让所有的特征都处于相似大小的范围，以让梯度下降得更快。

假设数据都是正数，$\mu$ 为数据的平均值，$s$ 为数据的标准差。

最大值归一化：$x=\dfrac{x}{x_{\max}}$ 。

均值归一化：$x=\dfrac{x - \mu}{x_{\max} - x_{\min}}$ 。

均值方差归一化：$x=\dfrac{x - \mu}{s}$ 。

### 学习率

画出 $J(\theta)$ 随训练轮次变化的曲线可以帮助判断梯度下降是否正常运行。如果 $J(\theta)$ 在某一段递增，通常是因为学习率设置得过大而引起，这个时候可以将学习率调小。

学习率对梯度下降的影响：

- 如果学习率过小，会导致梯度下降收敛得很慢。
- 如果学习率过大，会导致损失函数难以收敛，甚至发散。

在实践中，通常需要尝试多种学习率，找到最适合对应任务的学习率。

### 特征与多项式回归

有的时候可以将特征进行运算，以让线性回归模型拟合更加复杂的函数。

例如房屋的特征有长度和宽度，我们就可以将长度和宽度相乘，得到房屋的面积，作为一个新的特征输入模型，获得更好的效果。

在多项式回归模型中，每一项可以看成是原始特征进行幂运算之后得到的新特征，当然也可以进行开方、取对数等运算。

具体需要使用哪些特征，要如何组合特征，可以由自己自由选择，也可以使用算法去寻找，这也催生了特征工程这一学科。

### 正规方程

正规方程方法是一种计算参数 $\theta$ 最优值的解析方法，它可以一次性计算出参数 $\theta$ 的最优值，而不需要像梯度下降一样进行多次迭代。

假设特征矩阵 $X\in {\mathbb R}^{m\times n + 1}$ ，$m$ 表示有 $m$ 个样本，$n$ 表示有 $n$ 个特征，加一是加了一项 $x_0 = 1$ 。标签矩阵 $y\in {\mathbb R}^{m}$ 。那么最优参数 $\theta$ 可以通过如下公式进行计算：

$$\theta = (X^TX)^{-1}X^Ty$$

其中 $X^T$ 表示矩阵 $X$ 的转置，$(X^TX)^{-1}$ 表示矩阵 $X^TX$ 的逆。数学上可以证明该公式的正确性，在此不进行赘述。

梯度下降法与正规方程法的对比：

- 梯度下降法需要选择学习率，而正规方程法不需要。
- 梯度下降法需要通过多次迭代次啊能求解最优值，而正规方程法不需要迭代。
- 但是梯度下降法在特征数量很多（$n$ 很大）的时候依然有着优秀的表现，而正规方程法由于需要求解一个 $n\times n$ 的矩阵的逆，其时间复杂度为 $\Theta(n^3)$ ，当 $n$ 很大时，效率会非常低下。
- 梯度下降法比正规方程法更加泛用，正规方程法几乎只适用于线性回归。

> 在使用正规方程法时，如果 $X^TX$ 不可逆，应该怎么办？
> 
> 在遇到矩阵不可逆但是要求逆的情况时，可以通过求伪逆实现。
>
> 另外，矩阵不可逆通常是因为以下两种情况：
>
> 1. 不同的特征之间线性相关（特征多余）
> 2. 特征数量太多（例如特征数量大于样本数量）
>
> 要解决这些问题，一般采用删除某些特征，或者正则化的方式。

## 第五章 Octave 教程

人生苦短，我用 Python 。🐶

## 第六章 逻辑回归

### 分类

分类问题是指一类问题，它的输出 $y$ 只有两个离散的取值 $0$ 或 $1$ ，如判断邮件是否为垃圾邮件，网上交易是否属于诈骗，肿瘤是否属于恶性肿瘤等。其中，$y=1$ 的样本被称作为正例；$y=0$ 的样本被称作为负例。一般来说，我们会将希望找到的东西化为正例，但实际上正例与负例的划分是任意的。

线性回归算法不适用于分类问题，理由有如下两个：

1. 线性回归算法对噪声过于敏感。
2. 线性回归算法的输出值可能大于 $1$ 或者小于 $0$ ，这在所有标签都是 $0$ 或 $1$ 的问题中略显奇怪。

因此，本章将引入逻辑回归算法，该算法的输出值会在 $0\sim 1$ 之间。注意：虽然逻辑回归算法中有“回归”二字，但它实际上是一个分类算法。

### 模型表示

定义 $h_\theta(x)$ 表示在参数 $\theta$ 的影响下，给定特征 $x$ ，其属于正例 $y = 1$ 的概率。即：

$$h_\theta(x) = P(y = 1\ |\ x;\theta)$$

$h_\theta(x)$ 通过如下公式进行计算：

$$
\begin{align*}
    h_\theta(x) = g(\theta^T x) \\
    g(z) = \dfrac{1}{1 + e^{-z}}
\end{align*}
$$

这里的 $g(z)$ 被称作为 Sigmoid 函数或者 Logistic 函数（逻辑函数）。其具有如下特点：

1. 值域在 $0$ 到 $1$ 之间：逻辑函数的输出值范围在 $0$ 和 $1$ 之间，当 $x$ 趋近于正无穷大时，$f(x)$ 趋近于 $1$ ；当 $x$ 趋近于负无穷大时，$f(x)$ 趋近于 $0$。
2. 中心对称：逻辑函数在 $x=0$ 处对称，即 $f(0) = 0.5$ 。
3. 平滑性：逻辑函数的曲线平滑，无跳跃点或不连续点。

根据全概率公式，我们可以轻松地通过样本属于正例的概率推出样本属于负例的概率：

$$
\begin{align*}
    P(y = 0\ | \ x;\theta) + P(y = 1\ | \ x;\theta) = 1 \\
    P(y = 0\ | \ x;\theta) = 1 - P(y = 1\ | \ x;\theta)
\end{align*}
$$

### 决策边界

假设当 $h_\theta(x) \ge 0.5$ 时，预测 $y = 1$ ；当 $h_\theta(x) < 0.5$ 时，预测 $y = 0$ 。根据逻辑函数的性质，当 $z \ge 0$ 时，$g(z) \ge 0.5$ ，要使得 $h_\theta(x) \ge 0.5$ ，则要求 $\theta^T x \ge 0$ 。同理，若 $\theta^T x < 0$ ，那么 $h_\theta(x) < 0.5$ ，则会预测 $y = 0$ 。

通过上述分析，我们将 $y$ 的预测转换成了 $\theta^T x$ 值与 $0$ 的大小比较。对于二维的情况， $\theta^T x = 0$ 可以在平面上确定一条直线（曲线），这条直线（曲线）会将平面分成两个部分，其中一个部分中的点会被预测为正例，另外一个部分中的点会被预测为负例。这条直线（曲线）就是模型 $h_\theta(x)$ 的**决策边界**。

值得注意的是：决策边界是模型本身的性质，与数据集无关。数据集仅仅被用来优化模型。

### 损失函数

一般化地，将损失函数写成如下形式：

$$J(\theta) = \dfrac{1}{m}\sum\limits_{i=1}^m {\rm Cost}(h_\theta(x^{(i)}), y^{(i)})$$

对于线性回归，其损失函数为：

$${\rm Cost}(h_\theta(x), y) = \dfrac{1}{2}(h_\theta(x) - y)^2$$

这就是平方差损失函数。但是，对于逻辑回归，如果使用平方差损失函数，会导致损失函数非凸，因此需要另外一个损失函数——对数损失函数：

$$
{\rm Cost}(h_\theta(x), y) = 
\begin{cases}
    -\log(h_\theta(x)), & y = 1 \\
    -\log(1 - h_\theta(x)), & y = 0
\end{cases}
$$

对于 $y = 1$ 的情况，如果输出的 $h_\theta(x) = 1$ ，那么损失函数的值为 $0$ ，而如果输出的 $h_\theta(x) \to 0$ ，那么损失函数的值会趋向于无穷。这对 $y = 0$ 的情况也是类似的。

由于 $y$ 只会为 $0$ 或 $1$ ，因此可以将代价函数简化成如下形式：

$${\rm Cost}(h_\theta(x), y) = -y\log(h_\theta(x)) - (1-y)\log(1 - h_\theta(x))$$

### 梯度下降

通过损失函数对 $\theta$ 求偏导，可以得到梯度下降公式如下：

$$\theta_j := \theta_j - \alpha \sum\limits_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$$

可以发现，这个梯度下降公式与线性回归的下降公式是一致的。但是由于模型的定义发生了变化，所以逻辑回归和线性回归本质上其实是不同的算法。

### 高级优化

除了梯度下降算法，还有一些其他的高级算法可以用来寻找最优的参数值。如共轭梯度、BFGS、L-BFGS 等。这些算法通常不需要手动确定学习率，并且比梯度下降更快。但是其不足之处在于：相较于梯度下降算法，这些算法会有亿点点复杂。

### 多分类问题：一对多

多分类问题是指样本不止两类的情况，即 $y$ 可能为 $2, 3, 4, \cdots$ 。例如判断天气时晴天、多云、下雨或者下雪。

多分类问题的一对多方法的思想是：假设一共有 $n$ 个类别，训练 $n$ 个二分类器，其中第 $i$ 个分类器以第 $i$ 类样本为正例，其他所有样本为负例，输出样本属于第 $i$ 类的概率。即：

$$h^{(i)}_\theta(x) = P(y = i\ |\ x;\theta)\quad (i=1,2,3,\cdots)$$

对于一个新的样本 $x$，将其输入到所有 $n$ 个分类器中，取输出概率最大的那个分类器，作为 $x$ 的所属类别。即：

$$\max\limits_{i}h^{(i)}_\theta(x)$$

## 第七章 正则化

### 过拟合问题

如果特征数量非常庞大，那么模型可能会很好地拟合训练集，但是难以泛化到其他的样本中去，这就是过拟合问题。与过拟合相对，如果模型并不能很好地拟合训练集，就被称作为欠拟合。

高偏差与高方差的概念：

- 偏差（Bias）是指模型对于训练数据的错误偏离程度。当模型具有高偏差时，意味着模型对于训练数据的拟合程度较低，即欠拟合。
- 方差（Variance）是指模型对于不同训练数据集的响应程度。当模型具有高方差时，意味着模型对于训练数据的拟合程度较高，但在处理新的、未见过的数据时表现较差，即过拟合。

为了解决过拟合问题，常常有以下两种方式：

1. **减少特征的数量**：人工选择保留的特征，或者通过模型选择算法选择保留的特征。但是减少特征数量意味着丢失了一些用于解决问题的信息。
2. **正则化**：保留所有的特征，但是让 $\theta$ 的值处于一个很小的范围，这在有许多特征的问题中非常有效，每一个特征都对预测结果产生了一点点影响。

### 损失函数

正则化的思想在于通过在损失函数中加入对参数 $\theta$ 的乘法项，以让 $\theta$ 的值更小，更小的 $\theta$ 值就意味着更“简单”的模型，也意味着更难发生过拟合。

因此，运用了正则化思想的损失函数如下式所示（假设参数数量为 $n$）：

$$J(\theta) = \dfrac{1}{2m}\left[\sum\limits_{i=1}^m (h_\theta(x^{(i)} ) - y^{(i)})^2 + \lambda \sum\limits_{i=1}^n \theta^2_i\right]$$

后一个循环的下标从 $1$ 开始是因为一般不对 $\theta_0$ 进行正则化，但实际上对 $\theta_0$ 进行正则化也问题不大。

公式中的 $\lambda$ 是正则化系数，用于控制模型损失和正则化损失的权重，以引导学习算法更关注数据的拟合还是模型的简化。如果 $\lambda$ 被设置得过大，则可能导致以下后果：

- 算法设计得过于完美，把 $\lambda$ 设置得很大并不会造成什么影响（一般来说是想 peach）。
- 算法不再能防止过拟合。
- 算法甚至可能导致欠拟合。
- 梯度可能很难下降。

### 线性回归中的正则化

线性回归的求解有梯度下降和正规方程两种解法，因此对这两种情况分别进行讨论。

对于梯度下降法，损失函数为：

$$J(\theta) = \dfrac{1}{2m}\left[\sum\limits_{i=1}^m (h_\theta(x^{(i)} ) - y^{(i)})^2 + \lambda \sum\limits_{i=1}^n \theta^2_i\right]$$

因此梯度下降公式为：

$$
\begin{align*}
\theta_0 &:= \theta_0 - \alpha \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_0 \\
\theta_j &:= \theta_j - \alpha\left[ \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j - \dfrac{\lambda}{m}\theta_j \right]
\end{align*}
$$

注意，分情况讨论的原因是一般不对 $\theta_0$ 进行正则化，同时对下面的公式稍作变形，可以得到：

$$\theta_j := \theta_j(1 - \alpha\dfrac{\lambda}{m}) - \alpha \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j$$

由于一般 $\alpha$ 很小，而 $m$ 很大，因此 $1 - \alpha\dfrac{\lambda}{m}$ 会很接近于 $1$ 。于是我们就可以看到加入正则化之后的梯度下降相当于每一次先稍微减小一下 $\theta$ 的值，再进行常规的梯度下降，这与正则化想要减小参数值大小的思想是一致的。

对于正规方程法，最优 $\theta$ 求解公式为：

$$
\theta = \left(X^TX + \lambda
\begin{bmatrix}
    0 & \\
      & 1 \\
      &   & 1 \\
      &   &   & \ddots \\
      &   &   &   & 1
\end{bmatrix}
\right)^{-1}X^Ty$$

值得注意的是，在这个公式中，矩阵 $X^TX + \lambda\begin{bmatrix}0 & \\& 1 \\&   & 1 \\&   &   & \ddots \\&   &   &   & 1\end{bmatrix}$ 一定是可逆的。

### 逻辑回归中的正则化

在逻辑回归中，正则化的损失函数为：

$$J(\theta) = -\left[\sum\limits_{i=1}^m y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1 - h_\theta(x^{(i)}))\right] + \dfrac{\lambda}{2m}\sum\limits_{i=1}^n\theta^2_j$$

其梯度下降更新公式为：

$$
\begin{align*}
\theta_0 &:= \theta_0 - \alpha \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_0 \\
\theta_j &:= \theta_j - \alpha\left[ \dfrac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j - \dfrac{\lambda}{m}\theta_j \right]
\end{align*}
$$

可以看到，正则化的逻辑回归，其梯度下降公式与正则化线性回归的梯度下降公式是一样的。但是同样的：因为两种问题的模型在定义上就是不同的，所以本质上，这两种梯度下降是不一样的。
