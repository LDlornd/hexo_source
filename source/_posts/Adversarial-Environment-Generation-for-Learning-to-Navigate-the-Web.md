---
title: "[论文解读]Adversarial Environment Generation for Learning to Navigate the Web"
author: lornd
mathjax: true
date: 2023-08-17 14:38:00
url: aeg
tags:
---

论文地址：[Adversarial Environment Generation for Learning to Navigate the Web](https://arxiv.org/abs/2103.01991) 。

## 摘要

学习如何自动在网页中进行导航是一个困难的序列决策工作。状态空间和动作空间巨大，且具有组合特性，同时网站也是包含多个页面的动态环境。训练网页导航智能体的一个瓶颈是为训练环境提供一个可学习的课程，并且要求能够覆盖真实世界网站的多样性。因此，我们提出用对抗环境生成（Adversarial Environment Generation，AEG）方法生成具有挑战性的网页环境，以训练强化学习智能体。

- 我们提供了一个新的基准环境 gMiniWob ，使强化学习的对手能够使用组合原语学习生成任意复杂的网站。
- 为了训练这个对手，我们提出了一种新的技术，利用导航智能体获得的分数差来最大化遗憾。我们的实验结果表明：我们的方法在最小最大化遗憾 AEG 方面明显优于之前的方法。
- 遗憾目标训练对手设计一套对于导航智能体来说是“适中挑战”的课程。我们的实验结果表明：随着时间的推移，对手学会了生成逐渐变难的网页导航任务。
- 使用我们的技术训练的导航智能体学会了完成具有挑战性的，高维的网页导航任务，如表单填写，航班预定等。我们的实验结果表明：在一组具有挑战性的不可见测试环境中，通过我们提出的灵活的 b-PAIRED 技术训练的导航智能体的性能显著优于具有竞争力的自动化课程生成基线，包括了一个最先进的强化学习网页导航方法，并且在一些任务上达到了超过 $80\%$ 的准确率。

## 研究背景

本文的主要目的是训练一个强化学习智能体自动在网页中进行导航，将相关信息正确输入到未知的现实网站中。这种能力可以让用户发出如预定机票和发朋友圈等请求，并让强化学习智能体自动处理完成任务的细节。然而，现实网站的复杂性和多样性让这个任务非常困难。

为了让我们的智能体能够泛化到新的网站，我们让其直接在 DOM 树上进行操作，智能体必须正确选择并将信息填入到合适的 DOM 元素中。这让问题的状态-动作空间非常大。即使智能体能够在网页中导航到正确的表单，甚至选择了正确的元素，仍然会有很多的可以输入的值。为了解决这个问题，前人的研究利用了从专家演示进行动作克隆的方法。但是，这个方法十分脆弱，并且不能有效地泛化。为每个网站收集导航演示是不可能的，尤其在网页经常变化和更新的情况下。如果没有可用的演示数据，基于模仿学习的模型就不能够泛化到新的网站。

成功在大范围的现实网站中进行导航需要在大量可能的任务和环境中对智能体进行训练。问题在于如何创建这样一个任务分布，其不仅能够涵盖大多数现实任务，而且能够以可以让智能体进行学习课程的形式呈现。一种可能的做法是为网站人为设计一个提前定义好的课程。但是这种做法十分枯燥，耗时，易错，而且脆弱，设计者很容易忽略一些现实边缘情况。另外一种做法是使用域随机算法（Domain Randomization, DR）对网站的参数进行随机化，或者随着时间的推移自动增加一些参数以控制难度。然而，这些方法可能无法覆盖重要的测试案例，并且不能根据智能体当前的能力来调整参数配置的难度。

## 文章贡献

在本文中，我们利用最新的对抗环境生成（AEG）技术为富有挑战性的网页导航任务建立课程。具体地，我们训练一个对抗强化学习智能体学习在网站中创建新的页面，以利用当前正在学习网页导航的智能体的弱点。为了实验这个 AEG 网页设计技术，我们建立了一个新的框架 gMiniWoB ，以允许对抗智能体利用常见的设计基元构建网站，如导航栏、产品转盘、项目卡片、网络表单和购物车等。这个环境是开源的以促进后续的研究。

AEG 的目标是自动生成一个训练环境的课程，涵盖可能的网站空间，并由此实现到现实网站导航任务的泛化。然而，如果我们只是简单地应用一个最小最大对抗智能体，即对抗智能体试图让学习智能体的表现最糟糕，那么这种课程不太可能出现。这是因为对抗智能体的动机是创建一个尽可能难的网站，而不是根据学习智能体当前的技术水平调整网站难度。最近提出的一种 AEG 技术 PAIRED（Protagonist Antagonist Induced Regret Environment Design）训练对抗智能体去最大化遗憾值。我们通过两种新的算法改进了原始的 PAIRED 算法：

1. 我们提出了一种更加灵活的计算遗憾的方法，使得我们的算法梗不容易陷入局部最小值。
2. 我们引入了一个明确的预算机制，使得对抗智能体在学习智能体不能完成任务时，因为使得环境更复杂而受到惩罚；否则的话对抗智能体会因为使得环境变复杂而受到奖励。

本文总体有如下贡献：

- 一个新的基准环境 gMiniWoB ，通过允许利用组合设计基元来构建网站，增强 AEG 在网页导航中的应用。
- Flexible b-PAIRED 算法，可计算出一个更加稳定的对遗憾的估计，并直接鼓励对抗智能体根据学习智能体的表现调整环境的复杂程度。
- 实验结果表明：Flexible b-PAIRED 申城了一套包含越来越具有挑战性的网站的课程，并且训练出了能够在测试时成功泛化到复杂、未知网站的智能体。我们的方法明显超过了在最小最大遗憾 AEG 上的之前的工作，也包括了使用强化学习训练网页导航智能体的最先进的方法。

## 问题描述

### 网页导航问题

我们将网页导航问题定义为一个序列决策问题。我们训练一个智能体，其参数为一个网络 $\pi(a_t|s_t;\Theta_i)$ ，将输入的状态 $s_t$ 转换为输出的动作 $a_t$ 以最大化累计折损增益 $O = \sum\limits_{t=0}^T \gamma^tr_t$ ，其中 $r_t$ 是在 $t$ 时刻所获得的奖励，$\gamma$ 是折损因子，$T$ 是一个 episode 的长度。 **我们将网页和用户指令作为输入状态**。其中网页在每个时刻都会被动态更新，而用户指令在最开始就确定不变。我们使用 DOM 树表示网页，其中每个元素都用一个（属性，值）二元组的集合和一个特征数组进行表示。指令是字段的集合，其中字段都表示为（键，值）二元组。对于每个任务，键是固定的，但是值会基于用户的输入而动态变化。

**每个动作都被表示为（元素，字段）二元组，表示在元素上进行动作，并且将字段的值作为输入**，即将字段的值输入元素。智能体在每个 episode 的最后会收到一个**任务成功奖励** （$1.0$ 或 $-1.0$）；每当页面中的元素被更新，智能体会收到一个**基于潜力的奖励**；为了鼓励智能体进行高效地导航，每个时刻智能体都会收到一个**小惩罚**。

### PAIRED

AEG 训练一个对抗策略 $\pi_E$ 以设计环境使得学习智能体策略 $\pi_P$ 的表现尽可能坏。假设 $R^P_i = \sum\limits_{t=1}^T \gamma^tr^P_t$ 表示学习策略在轨迹 $i$ 上所获得的总奖励值。在最小最大 AEG 中，对抗策略的目标很简单：$-R^P$ 。因此，极小极大对抗策略有动机创造过于困难或不可能的环境，这可能使得学习策略无法学习。

## 解决方法

## 实验验证
